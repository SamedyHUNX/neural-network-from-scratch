{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a68b5d4-968b-424d-bd5a-6d9af6b2ba77",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76534fbf-70fc-4d3b-9c8a-c6f100d3518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc6b0bc-b65a-4a55-bb03-30a5211b2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "if not os.path.isfile(FILE):\n",
    "    print(f'Downloading {URL} and saving as {FILE}...')\n",
    "    urllib.request.urlretrieve(URL, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02aff84-0fb2-49ce-9d8d-04f80271649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping images...\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "print('Unzipping images...')\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "039a4af9-500f-4b2e-92ce-c93564a40485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '0', '7', '6', '1', '8', '4', '3', '2', '5']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "labels = os.listdir('fashion_mnist_images/train')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ce9f18-f920-417b-b3de-65838c877def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  49\n",
      "  135 182 150  59   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  78 255\n",
      "  220 212 219 255 246 191 155  87   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  57 206 215\n",
      "  203 191 203 212 216 217 220 211  15   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0  58 231 220 210\n",
      "  199 209 218 218 217 208 200 215  56   0]\n",
      " [  0   0   0   0   1   2   0   0   4   0   0   0   0 145 213 207 199 187\n",
      "  203 210 216 217 215 215 206 215 130   0]\n",
      " [  0   0   0   0   1   2   4   0   0   0   3 105 225 205 190 201 210 214\n",
      "  213 215 215 212 211 208 205 207 218   0]\n",
      " [  1   5   7   0   0   0   0   0  52 162 217 189 174 157 187 198 202 217\n",
      "  220 223 224 222 217 211 217 201 247  65]\n",
      " [  0   0   0   0   0   0  21  72 185 189 171 171 185 203 200 207 208 209\n",
      "  214 219 222 222 224 215 218 211 212 148]\n",
      " [  0  70 114 129 145 159 179 196 172 176 185 196 199 206 201 210 212 213\n",
      "  216 218 219 217 212 207 208 200 198 173]\n",
      " [  0 122 158 184 194 192 193 196 203 209 211 211 215 218 221 222 226 227\n",
      "  227 226 226 223 222 216 211 208 216 185]\n",
      " [ 21   0   0  12  48  82 123 152 170 184 195 211 225 232 233 237 242 242\n",
      "  240 240 238 236 222 209 200 193 185 106]\n",
      " [ 26  47  54  18   5   0   0   0   0   0   0   0   0   0   2   4   6   9\n",
      "    9   8   9   6   6   4   2   0   0   0]\n",
      " [  0  10  27  45  55  59  57  50  44  51  58  62  65  56  54  57  59  61\n",
      "   60  63  68  67  66  73  77  74  65  39]\n",
      " [  0   0   0   0   4   9  18  23  26  25  23  25  29  37  38  37  39  36\n",
      "   29  31  33  34  28  24  20  14   7   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_data = cv2.imread('fashion_mnist_images/train/7/0002.png',\n",
    "cv2.IMREAD_UNCHANGED)\n",
    "print(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2398a9f2-403b-40f6-95a5-974c56136716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a7fd5b-8856-4fc6-aacf-ca324905431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdTUlEQVR4nO3dbXCUZb7n8V/ngSZgpxUx6e4h5mQ8cGYWGOqIyMOCBLfMmLNDqczUQa2ahdoZS0egloquM4xVKzUviOOslC8YscadYqQGlReLQhWUGA8mjIXMQRdXinEZPASJS2IkA+kQoPN07QuWnmkec910808n30/VXUXuvv65r764k1/f6e5/h5xzTgAAGCiwngAAYOQihAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCmyHoCFxsYGNDx48cViUQUCoWspwMA8OScU1dXlxKJhAoKrn6tM+RC6Pjx46qoqLCeBgDgOrW0tGjChAlXHTPkQigSiUiS5uqfVKRi49kAAHz1qVcfaEf69/nV5CyEXn75Zf3qV79Sa2urJk+erJdeeknz5s27Zt2FP8EVqVhFIUIIAPLO/+9IOpinVHLywoTNmzdr5cqVevbZZ7V//37NmzdPtbW1OnbsWC4OBwDIUzkJobVr1+pHP/qRfvzjH+vb3/62XnrpJVVUVGj9+vW5OBwAIE9lPYR6enr08ccfq6amJmN/TU2N9uzZc8n4VCqlZDKZsQEARoash9CJEyfU39+v8vLyjP3l5eVqa2u7ZHx9fb2i0Wh645VxADBy5OzNqhc/IeWcu+yTVKtWrVJnZ2d6a2lpydWUAABDTNZfHTd+/HgVFhZectXT3t5+ydWRJIXDYYXD4WxPAwCQB7J+JTRq1ChNnz5dDQ0NGfsbGho0Z86cbB8OAJDHcvI+obq6Ov3whz/UXXfdpdmzZ+s3v/mNjh07pieeeCIXhwMA5KmchNDixYvV0dGhX/ziF2ptbdWUKVO0Y8cOVVZW5uJwAIA8FXLOOetJ/K1kMqloNKpqPUDHBADIQ32uV43aqs7OTpWWll51LB/lAAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADNZD6HVq1crFAplbLFYLNuHAQAMA0W5+KaTJ0/We++9l/66sLAwF4cBAOS5nIRQUVERVz8AgGvKyXNChw8fViKRUFVVlR5++GEdOXLkimNTqZSSyWTGBgAYGbIeQjNnztTGjRu1c+dOvfrqq2pra9OcOXPU0dFx2fH19fWKRqPpraKiIttTAgAMUSHnnMvlAbq7u3XHHXfomWeeUV1d3SW3p1IppVKp9NfJZFIVFRWq1gMqChXncmoAgBzoc71q1FZ1dnaqtLT0qmNz8pzQ3xo7dqymTp2qw4cPX/b2cDiscDic62kAAIagnL9PKJVK6bPPPlM8Hs/1oQAAeSbrIfT000+rqalJzc3N+uMf/6gf/OAHSiaTWrJkSbYPBQDIc1n/c9yXX36pRx55RCdOnNBtt92mWbNmae/evaqsrMz2oQAAeS7rIfTmm29m+1sCAIYpescBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwk/MPtQOGvVDIvya3H2h83fqr7/Su+bd/9v91MvnftXjXfPZlzLum4o1gv+rC2/cFqsPgcSUEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDF23geg3hjtgnl8wOVPev9eu9azZ13epd83VfxLvmn2P+na3/070nvGskqb2/27vm6S9rvWv2fvF33jU3NY71rpGk2175MFBdrnAlBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwNTIG/FQr519ygBqap2hneNf+xrinQsV459Q3vmmMp/wamX5672btmwuhT3jUv9d/kXSNJYwp6vGvm3/xn75rvjjvoXfPVlKh3jSS99z/v8K7p//rrQMcaDK6EAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmKGBKYa+G9lU9EY1I/0n/2akv1j3qndNS69/U1FJOt57s3fNmYFR3jWTb2r1rikv7vSuCarf+T9O/6rXv7FoR+9Y75qJJV9510jS2el/510z6h0amAIAhiFCCABgxjuEdu/erYULFyqRSCgUCuntt9/OuN05p9WrVyuRSKikpETV1dU6eND/szIAAMOfdwh1d3dr2rRpWrdu3WVvf+GFF7R27VqtW7dO+/btUywW03333aeurq7rniwAYHjxfmFCbW2tamtrL3ubc04vvfSSnn32WS1atEiS9Nprr6m8vFyvv/66Hn/88eubLQBgWMnqc0LNzc1qa2tTTU1Nel84HNb8+fO1Z8+ey9akUiklk8mMDQAwMmQ1hNra2iRJ5eXlGfvLy8vTt12svr5e0Wg0vVVUVGRzSgCAISwnr44LXfS+DufcJfsuWLVqlTo7O9NbS0tLLqYEABiCsvpm1VgsJun8FVE8Hk/vb29vv+Tq6IJwOKxwOJzNaQAA8kRWr4SqqqoUi8XU0NCQ3tfT06OmpibNmTMnm4cCAAwD3ldCp0+f1ueff57+urm5WZ988onGjRun22+/XStXrtSaNWs0ceJETZw4UWvWrNGYMWP06KOPZnXiAID85x1CH330kRYsWJD+uq6uTpK0ZMkS/e53v9Mzzzyjs2fP6sknn9TJkyc1c+ZMvfvuu4pEItmbNQBgWAg5d4M6Ng5SMplUNBpVtR5QUajYejrZE6QJZyjAX0sH+v1rAgoV+zesdL09OZiJrT9vmO5dU3f3e941nf0l3jUnem/yrpGk42f9m3D+/Vj/Jpe3FHd710QLz3rXFGrAu0aSel1hoDpfQZqeBm3k+kLDQu+aif9lr9f4PterRm1VZ2enSktLrzqW3nEAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADNZ/WRV2LuRna1vVEfsUJH/adryX+8OdKz6//w775qPz/R61/yxs8q7Jj7av2vyN8InvWskaWLJV941A87/MW1xqC/Acfw70p9xo71rzh/L/z6dG/Dv/j8g//t0oi/Yx+P8t+9u8a55Q4lAxxoMroQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYGboNTEOh89tgh4/yb9wZKiz0rpGkgTNn/IucC1DT718y4F8TVN9/mO5d0778rHfNym/t8q75l78Ea+74349817umvfMm75rvJI5716QG/H9cT/QGW4dJo9u8a0YX+jdyHR3yrykMDXjX9LhgP+td/SUBqsZ4V5QX+jen/T9n4941kvTELc3eNb9d9JDX+L7ec9K2rYMay5UQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM0O3galzkgbf9NOlUv6H8K4Y+ooqJnjX/GWuf40kxX/yb9415YV93jX/4+i/964JKj426V0zt8x/HYI0I72l2L9xbkHAs/xEn3/j05sKz3nXRAr8G9qOLvBvehrUrUWnvWt6AzRLDdKUNT7Kv+mpJP0+Odm75v8+6LfmA2d7pW2DG8uVEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADNDt4Gpp67Fs7xr/jI5FOhYvaX+TSEHov5NF0si/k1ZK2455V3zj6WfeNdI0v/uSHjXjCrs964J0lQ0VtLlXSNJowr8G6yGA9QEaXIZDtC4szjkv96SVBig8WmQYw0EeBx8bqDYu6bXBftV1zUw2rvmdL9/TZBGsx29Y71rpGDnXqLslNf4vu6UWgY5lishAIAZQggAYMY7hHbv3q2FCxcqkUgoFArp7bffzrh96dKlCoVCGdusWf5/KgMADH/eIdTd3a1p06Zp3bp1Vxxz//33q7W1Nb3t2LHjuiYJABievJ+tq62tVW1t7VXHhMNhxWKxwJMCAIwMOXlOqLGxUWVlZZo0aZIee+wxtbe3X3FsKpVSMpnM2AAAI0PWQ6i2tlabNm3Srl279OKLL2rfvn269957lUpd/uXG9fX1ikaj6a2ioiLbUwIADFFZf5/Q4sWL0/+eMmWK7rrrLlVWVmr79u1atGjRJeNXrVqlurq69NfJZJIgAoARIudvVo3H46qsrNThw4cve3s4HFY4HM71NAAAQ1DO3yfU0dGhlpYWxePxXB8KAJBnvK+ETp8+rc8//zz9dXNzsz755BONGzdO48aN0+rVq/X9739f8XhcR48e1c9//nONHz9eDz30UFYnDgDIf94h9NFHH2nBggXpry88n7NkyRKtX79eBw4c0MaNG3Xq1CnF43EtWLBAmzdvViQSyd6sAQDDgncIVVdXy7krN9vbuXPndU0oqK/v9G9GWvTN04GONbXsyi85v5LbRvsfq8/5/7U0SCPEoOaXf37tQReJFp31rgnahDOIMQX+TWNHB2gsOjrkX1MQGvCuGRvg/kjBGpgG0a9gTYR9nRsYFaguUuh/vp4q8G8sGqTp6a3F3d41kjS+2L+575dnbvYa3+t6Bj2W3nEAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADM5/2TVoIomJFRUMPhPXP3mTz/M4Wwy9dxyi3fN0e9M8q45+ff+nXU7/8G7RG6Cf6dgSRp/s39n8NhY/w6+5SVJ75p/GPOVd40kJYpPetfcWui/DkG6VI8J0K07Eurzrgmq9wZ1xO4N0F0+SI0k/bm3zLvmZJ9/F+32Hv+PujnVO8a7RpI+6rndu+bQOxO9xvenzg16LFdCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzAzZBqZuVLFcYfGgxw/M+0fvY/SXFHrXSFLRgS+9awr3/sm75tamlH+Nd8WNFaRV6tFANaUBqqSdAeu8Ffife6HCIDUBH2cWD/5n77oMDPjXFAS4T73+zV+Dcv3+98n19/sfaMC/ce55/nUT9LXX+D7Xq8ODHMuVEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADNDtoFp/5EvFAoNvolicbt/48ni8vHeNZLU+82Yf813JnjXuKKQd40C9IMsOhegeaKkgpR/Xf9o/1Ouf7T/Y6WB4gBrJ8kV+tcVnfFf9NCA864ZCDC3oA8zXUGAY/nfJbkAv4H6A/zfhgLMTVKg+9Qf9p/fQICf9VB/sDtVfNa/buyxbq/xof5z0v/aOqixXAkBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwM2QbmPrqTyb9i4LUSAod9q8JF4/yP87osHdNwU1jvWsU9p+bJLmiQu+a4gCNOzUQoCtrQbDHV67Y/0diYIz//1Ogh38BlkFBmp5KCvUFaMp6tsf/QIX+C+GC/N8GfLgdOtfrX9Pb53+gADXu3Dn/40hyZ8561wycOeN3DDf4deNKCABghhACAJjxCqH6+nrNmDFDkUhEZWVlevDBB3Xo0KGMMc45rV69WolEQiUlJaqurtbBgwezOmkAwPDgFUJNTU1atmyZ9u7dq4aGBvX19ammpkbd3X/9wKMXXnhBa9eu1bp167Rv3z7FYjHdd9996urqyvrkAQD5LeScC/qZg/r6669VVlampqYm3XPPPXLOKZFIaOXKlfrpT38qSUqlUiovL9cvf/lLPf7449f8nslkUtFoVNV6QEUen6w61IV4YYKkYJ8oygsTLhwoQA0vTDiPFyb8te4GvDChz/WqUVvV2dmp0tKrf+r1dT0n1NnZKUkaN26cJKm5uVltbW2qqalJjwmHw5o/f7727Nlz2e+RSqWUTCYzNgDAyBA4hJxzqqur09y5czVlyhRJUltbmySpvLw8Y2x5eXn6tovV19crGo2mt4qKiqBTAgDkmcAhtHz5cn366ad64403LrktFMr8E4Bz7pJ9F6xatUqdnZ3praWlJeiUAAB5JtCbVVesWKFt27Zp9+7dmjBhQnp/LBaTdP6KKB6Pp/e3t7dfcnV0QTgcVjgc4G/qAIC853Ul5JzT8uXLtWXLFu3atUtVVVUZt1dVVSkWi6mhoSG9r6enR01NTZozZ052ZgwAGDa8roSWLVum119/XVu3blUkEkk/zxONRlVSUqJQKKSVK1dqzZo1mjhxoiZOnKg1a9ZozJgxevTRR3NyBwAA+csrhNavXy9Jqq6uzti/YcMGLV26VJL0zDPP6OzZs3ryySd18uRJzZw5U++++64ikUhWJgwAGD6u631CuTBc3ycEACPFDXufEAAA14MQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmvEKovr5eM2bMUCQSUVlZmR588EEdOnQoY8zSpUsVCoUytlmzZmV10gCA4cErhJqamrRs2TLt3btXDQ0N6uvrU01Njbq7uzPG3X///WptbU1vO3bsyOqkAQDDQ5HP4HfeeSfj6w0bNqisrEwff/yx7rnnnvT+cDisWCyWnRkCAIat63pOqLOzU5I0bty4jP2NjY0qKyvTpEmT9Nhjj6m9vf2K3yOVSimZTGZsAICRIXAIOedUV1enuXPnasqUKen9tbW12rRpk3bt2qUXX3xR+/bt07333qtUKnXZ71NfX69oNJreKioqgk4JAJBnQs45F6Rw2bJl2r59uz744ANNmDDhiuNaW1tVWVmpN998U4sWLbrk9lQqlRFQyWRSFRUVqtYDKgoVB5kaAMBQn+tVo7aqs7NTpaWlVx3r9ZzQBStWrNC2bdu0e/fuqwaQJMXjcVVWVurw4cOXvT0cDiscDgeZBgAgz3mFkHNOK1as0FtvvaXGxkZVVVVds6ajo0MtLS2Kx+OBJwkAGJ68nhNatmyZfv/73+v1119XJBJRW1ub2tradPbsWUnS6dOn9fTTT+vDDz/U0aNH1djYqIULF2r8+PF66KGHcnIHAAD5y+tKaP369ZKk6urqjP0bNmzQ0qVLVVhYqAMHDmjjxo06deqU4vG4FixYoM2bNysSiWRt0gCA4cH7z3FXU1JSop07d17XhAAAIwe94wAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZoqsJ3Ax55wkqU+9kjOeDADAW596Jf319/nVDLkQ6urqkiR9oB3GMwEAXI+uri5Fo9Grjgm5wUTVDTQwMKDjx48rEokoFApl3JZMJlVRUaGWlhaVlpYazdAe63Ae63Ae63Ae63DeUFgH55y6urqUSCRUUHD1Z32G3JVQQUGBJkyYcNUxpaWlI/oku4B1OI91OI91OI91OM96Ha51BXQBL0wAAJghhAAAZvIqhMLhsJ577jmFw2HrqZhiHc5jHc5jHc5jHc7Lt3UYci9MAACMHHl1JQQAGF4IIQCAGUIIAGCGEAIAmMmrEHr55ZdVVVWl0aNHa/r06frDH/5gPaUbavXq1QqFQhlbLBaznlbO7d69WwsXLlQikVAoFNLbb7+dcbtzTqtXr1YikVBJSYmqq6t18OBBm8nm0LXWYenSpZecH7NmzbKZbI7U19drxowZikQiKisr04MPPqhDhw5ljBkJ58Ng1iFfzoe8CaHNmzdr5cqVevbZZ7V//37NmzdPtbW1OnbsmPXUbqjJkyertbU1vR04cMB6SjnX3d2tadOmad26dZe9/YUXXtDatWu1bt067du3T7FYTPfdd1+6D+Fwca11kKT7778/4/zYsWN49WBsamrSsmXLtHfvXjU0NKivr081NTXq7u5OjxkJ58Ng1kHKk/PB5Ym7777bPfHEExn7vvWtb7mf/exnRjO68Z577jk3bdo062mYkuTeeuut9NcDAwMuFou5559/Pr3v3LlzLhqNuldeecVghjfGxevgnHNLlixxDzzwgMl8rLS3tztJrqmpyTk3cs+Hi9fBufw5H/LiSqinp0cff/yxampqMvbX1NRoz549RrOycfjwYSUSCVVVVenhhx/WkSNHrKdkqrm5WW1tbRnnRjgc1vz580fcuSFJjY2NKisr06RJk/TYY4+pvb3deko51dnZKUkaN26cpJF7Ply8Dhfkw/mQFyF04sQJ9ff3q7y8PGN/eXm52trajGZ1482cOVMbN27Uzp079eqrr6qtrU1z5sxRR0eH9dTMXPj/H+nnhiTV1tZq06ZN2rVrl1588UXt27dP9957r1KplPXUcsI5p7q6Os2dO1dTpkyRNDLPh8utg5Q/58OQ66J9NRd/tINz7pJ9w1ltbW3631OnTtXs2bN1xx136LXXXlNdXZ3hzOyN9HNDkhYvXpz+95QpU3TXXXepsrJS27dv16JFiwxnlhvLly/Xp59+qg8++OCS20bS+XCldciX8yEvroTGjx+vwsLCSx7JtLe3X/KIZyQZO3aspk6dqsOHD1tPxcyFVwdyblwqHo+rsrJyWJ4fK1as0LZt2/T+++9nfPTLSDsfrrQOlzNUz4e8CKFRo0Zp+vTpamhoyNjf0NCgOXPmGM3KXiqV0meffaZ4PG49FTNVVVWKxWIZ50ZPT4+amppG9LkhSR0dHWppaRlW54dzTsuXL9eWLVu0a9cuVVVVZdw+Us6Ha63D5QzZ88HwRRFe3nzzTVdcXOx++9vfuj/96U9u5cqVbuzYse7o0aPWU7thnnrqKdfY2OiOHDni9u7d6773ve+5SCQy7Negq6vL7d+/3+3fv99JcmvXrnX79+93X3zxhXPOueeff95Fo1G3ZcsWd+DAAffII4+4eDzuksmk8cyz62rr0NXV5Z566im3Z88e19zc7N5//303e/Zs941vfGNYrcNPfvITF41GXWNjo2ttbU1vZ86cSY8ZCefDtdYhn86HvAkh55z79a9/7SorK92oUaPcnXfemfFyxJFg8eLFLh6Pu+LiYpdIJNyiRYvcwYMHraeVc++//76TdMm2ZMkS59z5l+U+99xzLhaLuXA47O655x534MAB20nnwNXW4cyZM66mpsbddtttrri42N1+++1uyZIl7tixY9bTzqrL3X9JbsOGDekxI+F8uNY65NP5wEc5AADM5MVzQgCA4YkQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAICZ/wc3rOGo/zEpowAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd05a57-69be-45d7-b9ee-369635761fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_data = cv2.imread('fashion_mnist_images/train/4/0011.png',\n",
    "cv2.IMREAD_UNCHANGED)\n",
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4d629f-13a4-4ce2-b258-00ceda3063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all the directories and create a list of labels\n",
    "labels = os.listdir('fashion_mnist_images/train')\n",
    "# Create lists for samples and labels\n",
    "X = []\n",
    "y = []\n",
    "# For each label folder\n",
    "for label in labels:\n",
    "    # And for each image in given folder\n",
    "    for file in os.listdir(os.path.join(\n",
    "    'fashion_mnist_images', 'train', label\n",
    "    )):\n",
    "        # Read the image\n",
    "        image = cv2.imread(os.path.join(\n",
    "        'fashion_mnist_images/train', label, file\n",
    "        ), cv2.IMREAD_UNCHANGED)\n",
    "        # And append it and a label to the lists\n",
    "        X.append(image)\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d1ca4-4316-4a43-96da-e82668aeeea6",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a560d6-ed06-4a33-bab4-b2139d584411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(os.path.join(\n",
    "            path, dataset, label, file\n",
    "            ), cv2.IMREAD_UNCHANGED)\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f11c0ac-5c5c-4c9f-b5f1-f1558b6aaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4e8217-68ec-43b1-b483-8307db7a382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a5a67-6d15-4378-8122-3a8b1f7acfcb",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f82454-ef69-4709-b6a8-d5cc2bf3ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "# Scale features\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf348d9-3795-4725-aa5d-464d2012ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "(2, 2)\n",
      "[1 2 3 4]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "example = np.array([[1,2],[3,4]])\n",
    "flattened = example.reshape(-1)\n",
    "print(example)\n",
    "print(example.shape)\n",
    "print(flattened)\n",
    "print(flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eefe3995-e560-4661-9eb4-a0176800ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to vectors\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32d596-9465-4c81-a28c-ba4a2f2dcbec",
   "metadata": {},
   "source": [
    "# Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546cce5c-cb00-485c-8608-f6a9f0513e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "print(keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f572a96-1483-4619-a10e-a2488809b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3048 19563 58303  8870 40228 31488 21860 56864   845 25770]\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "nnfs.init()\n",
    "np.random.shuffle(keys)\n",
    "print(keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a43686a-fb8b-433d-9fb0-7856e9109184",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a99fa8d-ddb8-4a43-89ce-576d968ecda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((X[4].reshape(28, 28))) # Reshape as image is a vector already\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb26eeb-5761-49ac-8371-e117d5df90a4",
   "metadata": {},
   "source": [
    "# Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7142f306-2791-4d81-a41e-4e39d17ebca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Calculate number of steps\n",
    "steps = X.shape[0] // BATCH_SIZE\n",
    "# Dividing rounds down. If there are some remaining data,\n",
    "# but not a full batch, this won't include it.\n",
    "# Add 1 to include the remaining samples in 1 more step.\n",
    "if steps * BATCH_SIZE < X.shape[0]:\n",
    "    steps += 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step in range(steps):\n",
    "        batch_X = X[step*BATCH_SIZE:(step+1)*BATCH_SIZE]\n",
    "        batch_y = y[step*BATCH_SIZE:(step+1)*BATCH_SIZE]\n",
    "        # Now we perform forward pass, loss calculation,\n",
    "        # backward pass and update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3033f8-9b50-4200-a358-4c74df061fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 'layer'\n",
    "class Layer_Input:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a769ed46-38d2-413b-b54f-a1b73356e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0,\n",
    "                 weight_regularizer_l2=0, bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "    # Set weights and biases in a layer instance\n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    # Get weights and biases\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f514da9-b47f-4188-9083-726f326da4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    # init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b83bde35-5cc9-4093-9d8d-8acc72ded5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu activation\n",
    "class Activation_ReLU:     \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable\n",
    "        # Let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Prediction\n",
    "    def predictions(self, outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "983a9ca8-0bc1-4b3c-a038-59de2540d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitiated array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0927686e-2686-40f6-bc11-b0dbd17497cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fd15714-5ce6-4438-801b-7d451df53430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3e3c449-c130-4029-8273-82e36ee70ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates with momentum\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            # Vanilla SGD updates (no momentum)\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52fec3d7-ae52-4205-9788-f33db58a376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1.0, decay=0, eps=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.eps = eps\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeroes\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD paramter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.eps)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.eps)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dab2ae2-4bd7-4f41-9f71-d887307b5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0, eps=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.eps = eps\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeroes\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD paramter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.eps)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.eps)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f8fcdcd-370a-4771-ac78-928a5104cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeroes\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "    \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "    \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "        weight_momentums_corrected / \\\n",
    "        (np.sqrt(weight_cache_corrected) +\n",
    "        self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "        bias_momentums_corrected / \\\n",
    "        (np.sqrt(bias_cache_corrected) +\n",
    "        self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4076be4-5a40-4502-bf8c-16e144ede82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    def regularization_loss(self):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        # itrate all trainable layers\n",
    "        for layer in self.trainable_layers:\n",
    "        \n",
    "            # L1 regularization - weights\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "    \n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "    \n",
    "            # L1 regularization - biases\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(np.abs(layer.biases * layer.biases))\n",
    "    \n",
    "        return regularization_loss\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "    \n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return loss and regularization loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd4c9d1c-53b4-4902-abfe-4f980e1205ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    "        # Ensure any of the probabilities is neither 0 or 1\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis =1\n",
    "            )\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We will use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "768bde2a-3455-4bcd-9896-66cfe36fe081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossEntropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        clipped_values = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -((y_true / clipped_values) - (1 - y_true) / (1 - clipped_values)) / outputs\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd6ca86c-1367-44eb-829b-4bc57433212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We will use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normlaize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8337dce-6d2f-4283-8eec-e67e7f32efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean(np.abs(y_pred - y_true), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We will use the first sammple to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4980902-937b-404c-83fa-28ec7187603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "    # Calculates an accuracy\n",
    "    # given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        \n",
    "        # Calculate batch accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        # Update accumulated values (FIXED PART)\n",
    "        self.accumulated_sum += np.sum(comparisons)  # Sum of correct predictions\n",
    "        self.accumulated_count += len(comparisons)   # Total samples in batch\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "        if self.accumulated_count == 0:\n",
    "            return 0.0 \n",
    "        return self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45472a73-79c2-488d-9d8a-755341398db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e404ca0-0744-4033-a8bb-b97425d97a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        # Create precision property\n",
    "        self.precision = None\n",
    "\n",
    "    # Calculate precision value\n",
    "    # baed on passed in ground truth\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    # Compare predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "521eb11d-c315-42ba-bb2f-a4fc1fa88348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a762c3f7-6070-463e-a320-d3452410b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Placeholder for softmax classifier/loss combination\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Set loss, optimizer, and accuracy\n",
    "    def set(self, *, loss=None, optimizer=None, accuracy=None):\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy is not None:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "    # Evaluates the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "        # Default value is batch size is not being set\n",
    "        validation_steps = 1\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "    \n",
    "        # Reset accumulated values in loss\n",
    "        # and accuracy objects\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "    \n",
    "        # Iterate over steps\n",
    "        for step in range(validation_steps):\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "    \n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "                batch_y = y_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "    \n",
    "            # Perform the forward pass\n",
    "            output = self.forward(batch_X, training=False)\n",
    "    \n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "    \n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "            # Get and print validation loss and accuracy\n",
    "            validation_loss = self.loss.calculate_accumulated()\n",
    "            validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "            # Print a summary\n",
    "            print(f'validation, ' +\n",
    "                f'acc: {validation_accuracy:.3f}, ' +\n",
    "                f'loss: {validation_loss:.3f}')\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None):\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not set\n",
    "        train_steps = 1\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f'epoch: {epoch}')\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            for step in range(train_steps):\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size:(step + 1) * batch_size]\n",
    "                    batch_y = y[step * batch_size:(step + 1) * batch_size]\n",
    "\n",
    "                output = self.forward(batch_X, training=True)\n",
    "                data_loss, regularization_loss = self.loss.calculate(output, batch_y, include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            if validation_data is not None:\n",
    "                self.evaluate(*validation_data, batch_size=batch_size)\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "        layer_count = len(self.layers)\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "        self.output_layer_activation = self.layers[-1]\n",
    "        # Update loss object with trainable layers\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X, training):\n",
    "        self.input_layer.forward(X, training)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "        return self.layers[-1].output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, output, y):\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "\n",
    "        self.loss.backward(output, y)\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "    # Retrieves and returns parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        # Create a list for parameters\n",
    "        parameters = []\n",
    "        # Iterable trainable layers and get their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "        # Return a list\n",
    "        return parameters\n",
    "\n",
    "    # Updates the model with new parameters\n",
    "    def set_parameters(self, parameters):\n",
    "        # Iterate over the parameters and layers\n",
    "        # and update each layers with each set of the parameters\n",
    "        for parameter_set, layer in zip(parameters, self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "\n",
    "    # Saves the parameters to a file\n",
    "    def save_parameters(self, path):\n",
    "        # Open a file in the binary-write mode\n",
    "        # and save parameters to it\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)\n",
    "\n",
    "    # Loads the weights and updates a model instance with them\n",
    "    def load_parameters(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            self.set_parameters(pickle.load(f))\n",
    "\n",
    "    # Saves the model\n",
    "    def save(self, path):\n",
    "        # Make a deep copy of current model instance\n",
    "        model = copy.deepcopy(self)\n",
    "\n",
    "        # Reset accumulated values in loss and accuracy objects\n",
    "        model.loss.new_pass()\n",
    "        model.accuracy.new_pass()\n",
    "\n",
    "        # Remove data from the input layer\n",
    "        # and gradients from the loss object\n",
    "        model.input_layer.__dict__.pop('output', None)\n",
    "        model.loss.__dict__.pop('dinputs', None)\n",
    "\n",
    "        # For each layer remove inputs, output and dinputs properties\n",
    "        for layer in model.layers:\n",
    "            for property in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:\n",
    "                layer.__dict__.pop(property, None)\n",
    "            \n",
    "        # Open a file in the binary-write mode and save the model\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    # Loads and returns a model\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        # Open file in the binary-read mode, load a model\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        # Return a model\n",
    "        return model\n",
    "\n",
    "    # Predicts on the samples\n",
    "    def predict(self, X, *, batch_size=None):\n",
    "        \n",
    "        # Default value if batch size is not being set\n",
    "        prediction_steps = 1\n",
    "        \n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            prediction_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data, but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if prediction_steps * batch_size < len(X):\n",
    "                prediction_steps += 1\n",
    "            \n",
    "        # Model outputs\n",
    "        output = []\n",
    "        \n",
    "        # Iterate over steps\n",
    "        for step in range(prediction_steps):\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X\n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "            # Perform the forward pass\n",
    "            batch_output = self.forward(batch_X, training=False)\n",
    "            # Append batch prediction to the list of predictions\n",
    "            output.append(batch_output)\n",
    "        # Stack and return results\n",
    "        return np.vstack(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc832b-949c-4283-92e5-1308e5767110",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc84cef6-1448-4c42-a1e4-a126697eaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
    "127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "422d42de-3a72-487a-aaf7-a8a51ef8629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.172, loss: 2.302 (data_loss: 2.302, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.719, loss: 0.743 (data_loss: 0.743, reg_loss: 0.000), lr: 0.0009950248756218907\n",
      "step: 200, acc: 0.789, loss: 0.702 (data_loss: 0.702, reg_loss: 0.000), lr: 0.0009900990099009901\n",
      "step: 300, acc: 0.812, loss: 0.468 (data_loss: 0.468, reg_loss: 0.000), lr: 0.0009852216748768474\n",
      "step: 400, acc: 0.820, loss: 0.593 (data_loss: 0.593, reg_loss: 0.000), lr: 0.000980392156862745\n",
      "step: 468, acc: 0.823, loss: 0.491 (data_loss: 0.491, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "training, acc: 0.718, loss: 0.749 (data_loss: 0.749, reg_loss: 0.000), lr: 0.0009771350400625367\n",
      "validation, acc: 0.945, loss: 0.139\n",
      "validation, acc: 0.949, loss: 0.139\n",
      "validation, acc: 0.930, loss: 0.170\n",
      "validation, acc: 0.934, loss: 0.170\n",
      "validation, acc: 0.944, loss: 0.151\n",
      "validation, acc: 0.948, loss: 0.145\n",
      "validation, acc: 0.946, loss: 0.155\n",
      "validation, acc: 0.941, loss: 0.170\n",
      "validation, acc: 0.920, loss: 0.236\n",
      "validation, acc: 0.904, loss: 0.284\n",
      "validation, acc: 0.893, loss: 0.311\n",
      "validation, acc: 0.886, loss: 0.334\n",
      "validation, acc: 0.876, loss: 0.371\n",
      "validation, acc: 0.869, loss: 0.389\n",
      "validation, acc: 0.861, loss: 0.408\n",
      "validation, acc: 0.857, loss: 0.416\n",
      "validation, acc: 0.862, loss: 0.403\n",
      "validation, acc: 0.862, loss: 0.396\n",
      "validation, acc: 0.863, loss: 0.394\n",
      "validation, acc: 0.865, loss: 0.385\n",
      "validation, acc: 0.867, loss: 0.378\n",
      "validation, acc: 0.867, loss: 0.374\n",
      "validation, acc: 0.869, loss: 0.371\n",
      "validation, acc: 0.863, loss: 0.383\n",
      "validation, acc: 0.852, loss: 0.408\n",
      "validation, acc: 0.842, loss: 0.432\n",
      "validation, acc: 0.833, loss: 0.451\n",
      "validation, acc: 0.821, loss: 0.474\n",
      "validation, acc: 0.811, loss: 0.494\n",
      "validation, acc: 0.801, loss: 0.515\n",
      "validation, acc: 0.793, loss: 0.532\n",
      "validation, acc: 0.795, loss: 0.528\n",
      "validation, acc: 0.801, loss: 0.517\n",
      "validation, acc: 0.804, loss: 0.511\n",
      "validation, acc: 0.809, loss: 0.500\n",
      "validation, acc: 0.812, loss: 0.492\n",
      "validation, acc: 0.817, loss: 0.484\n",
      "validation, acc: 0.820, loss: 0.476\n",
      "validation, acc: 0.823, loss: 0.472\n",
      "validation, acc: 0.824, loss: 0.471\n",
      "validation, acc: 0.825, loss: 0.468\n",
      "validation, acc: 0.827, loss: 0.466\n",
      "validation, acc: 0.828, loss: 0.462\n",
      "validation, acc: 0.830, loss: 0.459\n",
      "validation, acc: 0.832, loss: 0.456\n",
      "validation, acc: 0.833, loss: 0.456\n",
      "validation, acc: 0.834, loss: 0.457\n",
      "validation, acc: 0.827, loss: 0.469\n",
      "validation, acc: 0.825, loss: 0.476\n",
      "validation, acc: 0.821, loss: 0.485\n",
      "validation, acc: 0.817, loss: 0.493\n",
      "validation, acc: 0.815, loss: 0.499\n",
      "validation, acc: 0.811, loss: 0.507\n",
      "validation, acc: 0.807, loss: 0.515\n",
      "validation, acc: 0.805, loss: 0.520\n",
      "validation, acc: 0.806, loss: 0.521\n",
      "validation, acc: 0.806, loss: 0.521\n",
      "validation, acc: 0.807, loss: 0.521\n",
      "validation, acc: 0.807, loss: 0.521\n",
      "validation, acc: 0.808, loss: 0.520\n",
      "validation, acc: 0.808, loss: 0.521\n",
      "validation, acc: 0.809, loss: 0.522\n",
      "validation, acc: 0.808, loss: 0.524\n",
      "validation, acc: 0.807, loss: 0.526\n",
      "validation, acc: 0.804, loss: 0.533\n",
      "validation, acc: 0.804, loss: 0.536\n",
      "validation, acc: 0.803, loss: 0.539\n",
      "validation, acc: 0.802, loss: 0.542\n",
      "validation, acc: 0.802, loss: 0.544\n",
      "validation, acc: 0.801, loss: 0.546\n",
      "validation, acc: 0.801, loss: 0.547\n",
      "validation, acc: 0.800, loss: 0.547\n",
      "validation, acc: 0.800, loss: 0.546\n",
      "validation, acc: 0.801, loss: 0.545\n",
      "validation, acc: 0.801, loss: 0.543\n",
      "validation, acc: 0.802, loss: 0.541\n",
      "validation, acc: 0.802, loss: 0.540\n",
      "validation, acc: 0.802, loss: 0.539\n",
      "validation, acc: 0.802, loss: 0.538\n",
      "epoch: 2\n",
      "step: 0, acc: 0.836, loss: 0.489 (data_loss: 0.489, reg_loss: 0.000), lr: 0.0009770873027505008\n",
      "step: 100, acc: 0.828, loss: 0.451 (data_loss: 0.451, reg_loss: 0.000), lr: 0.000972337012008362\n",
      "step: 200, acc: 0.812, loss: 0.518 (data_loss: 0.518, reg_loss: 0.000), lr: 0.0009676326866321544\n",
      "step: 300, acc: 0.875, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.0009629736626703259\n",
      "step: 400, acc: 0.844, loss: 0.525 (data_loss: 0.525, reg_loss: 0.000), lr: 0.0009583592888974076\n",
      "step: 468, acc: 0.865, loss: 0.396 (data_loss: 0.396, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "training, acc: 0.832, loss: 0.467 (data_loss: 0.467, reg_loss: 0.000), lr: 0.0009552466924583273\n",
      "validation, acc: 0.945, loss: 0.137\n",
      "validation, acc: 0.957, loss: 0.127\n",
      "validation, acc: 0.945, loss: 0.147\n",
      "validation, acc: 0.941, loss: 0.149\n",
      "validation, acc: 0.950, loss: 0.133\n",
      "validation, acc: 0.956, loss: 0.128\n",
      "validation, acc: 0.954, loss: 0.141\n",
      "validation, acc: 0.951, loss: 0.155\n",
      "validation, acc: 0.931, loss: 0.216\n",
      "validation, acc: 0.913, loss: 0.263\n",
      "validation, acc: 0.902, loss: 0.294\n",
      "validation, acc: 0.894, loss: 0.316\n",
      "validation, acc: 0.882, loss: 0.359\n",
      "validation, acc: 0.876, loss: 0.375\n",
      "validation, acc: 0.869, loss: 0.391\n",
      "validation, acc: 0.865, loss: 0.401\n",
      "validation, acc: 0.870, loss: 0.386\n",
      "validation, acc: 0.873, loss: 0.374\n",
      "validation, acc: 0.875, loss: 0.369\n",
      "validation, acc: 0.878, loss: 0.358\n",
      "validation, acc: 0.880, loss: 0.350\n",
      "validation, acc: 0.881, loss: 0.344\n",
      "validation, acc: 0.882, loss: 0.340\n",
      "validation, acc: 0.878, loss: 0.349\n",
      "validation, acc: 0.869, loss: 0.373\n",
      "validation, acc: 0.858, loss: 0.396\n",
      "validation, acc: 0.851, loss: 0.411\n",
      "validation, acc: 0.841, loss: 0.430\n",
      "validation, acc: 0.831, loss: 0.449\n",
      "validation, acc: 0.823, loss: 0.468\n",
      "validation, acc: 0.816, loss: 0.482\n",
      "validation, acc: 0.818, loss: 0.479\n",
      "validation, acc: 0.822, loss: 0.469\n",
      "validation, acc: 0.825, loss: 0.464\n",
      "validation, acc: 0.830, loss: 0.452\n",
      "validation, acc: 0.833, loss: 0.445\n",
      "validation, acc: 0.836, loss: 0.438\n",
      "validation, acc: 0.839, loss: 0.432\n",
      "validation, acc: 0.842, loss: 0.429\n",
      "validation, acc: 0.844, loss: 0.425\n",
      "validation, acc: 0.846, loss: 0.420\n",
      "validation, acc: 0.848, loss: 0.414\n",
      "validation, acc: 0.851, loss: 0.407\n",
      "validation, acc: 0.853, loss: 0.403\n",
      "validation, acc: 0.856, loss: 0.398\n",
      "validation, acc: 0.858, loss: 0.394\n",
      "validation, acc: 0.859, loss: 0.393\n",
      "validation, acc: 0.855, loss: 0.401\n",
      "validation, acc: 0.853, loss: 0.406\n",
      "validation, acc: 0.851, loss: 0.412\n",
      "validation, acc: 0.850, loss: 0.417\n",
      "validation, acc: 0.849, loss: 0.420\n",
      "validation, acc: 0.847, loss: 0.426\n",
      "validation, acc: 0.844, loss: 0.433\n",
      "validation, acc: 0.843, loss: 0.436\n",
      "validation, acc: 0.843, loss: 0.437\n",
      "validation, acc: 0.843, loss: 0.437\n",
      "validation, acc: 0.843, loss: 0.437\n",
      "validation, acc: 0.844, loss: 0.437\n",
      "validation, acc: 0.844, loss: 0.436\n",
      "validation, acc: 0.844, loss: 0.438\n",
      "validation, acc: 0.844, loss: 0.438\n",
      "validation, acc: 0.843, loss: 0.441\n",
      "validation, acc: 0.842, loss: 0.444\n",
      "validation, acc: 0.840, loss: 0.452\n",
      "validation, acc: 0.838, loss: 0.456\n",
      "validation, acc: 0.837, loss: 0.461\n",
      "validation, acc: 0.835, loss: 0.465\n",
      "validation, acc: 0.834, loss: 0.468\n",
      "validation, acc: 0.832, loss: 0.471\n",
      "validation, acc: 0.832, loss: 0.472\n",
      "validation, acc: 0.831, loss: 0.472\n",
      "validation, acc: 0.832, loss: 0.471\n",
      "validation, acc: 0.832, loss: 0.470\n",
      "validation, acc: 0.832, loss: 0.468\n",
      "validation, acc: 0.833, loss: 0.465\n",
      "validation, acc: 0.834, loss: 0.464\n",
      "validation, acc: 0.833, loss: 0.464\n",
      "validation, acc: 0.834, loss: 0.463\n",
      "epoch: 3\n",
      "step: 0, acc: 0.859, loss: 0.445 (data_loss: 0.445, reg_loss: 0.000), lr: 0.0009552010698251983\n",
      "step: 100, acc: 0.852, loss: 0.417 (data_loss: 0.417, reg_loss: 0.000), lr: 0.0009506607091928891\n",
      "step: 200, acc: 0.836, loss: 0.469 (data_loss: 0.469, reg_loss: 0.000), lr: 0.0009461633077869241\n",
      "step: 300, acc: 0.914, loss: 0.325 (data_loss: 0.325, reg_loss: 0.000), lr: 0.0009417082587814295\n",
      "step: 400, acc: 0.867, loss: 0.488 (data_loss: 0.488, reg_loss: 0.000), lr: 0.0009372949667260287\n",
      "step: 468, acc: 0.865, loss: 0.346 (data_loss: 0.346, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "training, acc: 0.851, loss: 0.414 (data_loss: 0.414, reg_loss: 0.000), lr: 0.000934317481080071\n",
      "validation, acc: 0.945, loss: 0.135\n",
      "validation, acc: 0.953, loss: 0.119\n",
      "validation, acc: 0.948, loss: 0.138\n",
      "validation, acc: 0.947, loss: 0.139\n",
      "validation, acc: 0.956, loss: 0.123\n",
      "validation, acc: 0.960, loss: 0.118\n",
      "validation, acc: 0.960, loss: 0.134\n",
      "validation, acc: 0.955, loss: 0.147\n",
      "validation, acc: 0.937, loss: 0.197\n",
      "validation, acc: 0.920, loss: 0.237\n",
      "validation, acc: 0.912, loss: 0.265\n",
      "validation, acc: 0.904, loss: 0.283\n",
      "validation, acc: 0.892, loss: 0.322\n",
      "validation, acc: 0.888, loss: 0.334\n",
      "validation, acc: 0.881, loss: 0.347\n",
      "validation, acc: 0.878, loss: 0.358\n",
      "validation, acc: 0.882, loss: 0.344\n",
      "validation, acc: 0.885, loss: 0.333\n",
      "validation, acc: 0.888, loss: 0.329\n",
      "validation, acc: 0.890, loss: 0.319\n",
      "validation, acc: 0.892, loss: 0.312\n",
      "validation, acc: 0.893, loss: 0.307\n",
      "validation, acc: 0.894, loss: 0.304\n",
      "validation, acc: 0.889, loss: 0.313\n",
      "validation, acc: 0.879, loss: 0.341\n",
      "validation, acc: 0.867, loss: 0.365\n",
      "validation, acc: 0.860, loss: 0.379\n",
      "validation, acc: 0.851, loss: 0.399\n",
      "validation, acc: 0.842, loss: 0.419\n",
      "validation, acc: 0.833, loss: 0.438\n",
      "validation, acc: 0.826, loss: 0.454\n",
      "validation, acc: 0.829, loss: 0.450\n",
      "validation, acc: 0.833, loss: 0.441\n",
      "validation, acc: 0.835, loss: 0.435\n",
      "validation, acc: 0.840, loss: 0.425\n",
      "validation, acc: 0.843, loss: 0.418\n",
      "validation, acc: 0.846, loss: 0.411\n",
      "validation, acc: 0.848, loss: 0.406\n",
      "validation, acc: 0.851, loss: 0.404\n",
      "validation, acc: 0.853, loss: 0.399\n",
      "validation, acc: 0.855, loss: 0.394\n",
      "validation, acc: 0.858, loss: 0.387\n",
      "validation, acc: 0.860, loss: 0.380\n",
      "validation, acc: 0.863, loss: 0.376\n",
      "validation, acc: 0.865, loss: 0.371\n",
      "validation, acc: 0.868, loss: 0.366\n",
      "validation, acc: 0.869, loss: 0.365\n",
      "validation, acc: 0.865, loss: 0.374\n",
      "validation, acc: 0.863, loss: 0.379\n",
      "validation, acc: 0.861, loss: 0.385\n",
      "validation, acc: 0.859, loss: 0.390\n",
      "validation, acc: 0.858, loss: 0.393\n",
      "validation, acc: 0.855, loss: 0.399\n",
      "validation, acc: 0.853, loss: 0.406\n",
      "validation, acc: 0.852, loss: 0.410\n",
      "validation, acc: 0.851, loss: 0.411\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.853, loss: 0.410\n",
      "validation, acc: 0.853, loss: 0.411\n",
      "validation, acc: 0.853, loss: 0.411\n",
      "validation, acc: 0.852, loss: 0.414\n",
      "validation, acc: 0.852, loss: 0.417\n",
      "validation, acc: 0.849, loss: 0.423\n",
      "validation, acc: 0.847, loss: 0.427\n",
      "validation, acc: 0.847, loss: 0.431\n",
      "validation, acc: 0.845, loss: 0.434\n",
      "validation, acc: 0.844, loss: 0.437\n",
      "validation, acc: 0.843, loss: 0.440\n",
      "validation, acc: 0.842, loss: 0.441\n",
      "validation, acc: 0.843, loss: 0.440\n",
      "validation, acc: 0.843, loss: 0.439\n",
      "validation, acc: 0.843, loss: 0.438\n",
      "validation, acc: 0.843, loss: 0.436\n",
      "validation, acc: 0.844, loss: 0.434\n",
      "validation, acc: 0.845, loss: 0.433\n",
      "validation, acc: 0.845, loss: 0.432\n",
      "validation, acc: 0.845, loss: 0.431\n",
      "epoch: 4\n",
      "step: 0, acc: 0.859, loss: 0.423 (data_loss: 0.423, reg_loss: 0.000), lr: 0.0009342738356612324\n",
      "step: 100, acc: 0.852, loss: 0.383 (data_loss: 0.383, reg_loss: 0.000), lr: 0.0009299297903008323\n",
      "step: 200, acc: 0.828, loss: 0.434 (data_loss: 0.434, reg_loss: 0.000), lr: 0.0009256259545517657\n",
      "step: 300, acc: 0.938, loss: 0.299 (data_loss: 0.299, reg_loss: 0.000), lr: 0.0009213617727000506\n",
      "step: 400, acc: 0.867, loss: 0.466 (data_loss: 0.466, reg_loss: 0.000), lr: 0.0009171366992250195\n",
      "step: 468, acc: 0.896, loss: 0.321 (data_loss: 0.321, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "training, acc: 0.862, loss: 0.385 (data_loss: 0.385, reg_loss: 0.000), lr: 0.0009142857142857143\n",
      "validation, acc: 0.945, loss: 0.133\n",
      "validation, acc: 0.957, loss: 0.115\n",
      "validation, acc: 0.948, loss: 0.134\n",
      "validation, acc: 0.949, loss: 0.134\n",
      "validation, acc: 0.958, loss: 0.118\n",
      "validation, acc: 0.961, loss: 0.113\n",
      "validation, acc: 0.961, loss: 0.132\n",
      "validation, acc: 0.957, loss: 0.145\n",
      "validation, acc: 0.938, loss: 0.192\n",
      "validation, acc: 0.920, loss: 0.228\n",
      "validation, acc: 0.914, loss: 0.256\n",
      "validation, acc: 0.908, loss: 0.274\n",
      "validation, acc: 0.896, loss: 0.312\n",
      "validation, acc: 0.892, loss: 0.323\n",
      "validation, acc: 0.885, loss: 0.334\n",
      "validation, acc: 0.880, loss: 0.344\n",
      "validation, acc: 0.885, loss: 0.331\n",
      "validation, acc: 0.888, loss: 0.320\n",
      "validation, acc: 0.890, loss: 0.315\n",
      "validation, acc: 0.893, loss: 0.305\n",
      "validation, acc: 0.895, loss: 0.299\n",
      "validation, acc: 0.896, loss: 0.293\n",
      "validation, acc: 0.897, loss: 0.290\n",
      "validation, acc: 0.893, loss: 0.298\n",
      "validation, acc: 0.883, loss: 0.325\n",
      "validation, acc: 0.872, loss: 0.348\n",
      "validation, acc: 0.866, loss: 0.361\n",
      "validation, acc: 0.857, loss: 0.380\n",
      "validation, acc: 0.850, loss: 0.398\n",
      "validation, acc: 0.842, loss: 0.416\n",
      "validation, acc: 0.835, loss: 0.431\n",
      "validation, acc: 0.837, loss: 0.428\n",
      "validation, acc: 0.841, loss: 0.419\n",
      "validation, acc: 0.843, loss: 0.413\n",
      "validation, acc: 0.848, loss: 0.403\n",
      "validation, acc: 0.850, loss: 0.396\n",
      "validation, acc: 0.853, loss: 0.389\n",
      "validation, acc: 0.855, loss: 0.386\n",
      "validation, acc: 0.858, loss: 0.383\n",
      "validation, acc: 0.860, loss: 0.379\n",
      "validation, acc: 0.862, loss: 0.373\n",
      "validation, acc: 0.865, loss: 0.366\n",
      "validation, acc: 0.867, loss: 0.360\n",
      "validation, acc: 0.869, loss: 0.356\n",
      "validation, acc: 0.872, loss: 0.351\n",
      "validation, acc: 0.874, loss: 0.347\n",
      "validation, acc: 0.875, loss: 0.345\n",
      "validation, acc: 0.872, loss: 0.353\n",
      "validation, acc: 0.870, loss: 0.358\n",
      "validation, acc: 0.868, loss: 0.364\n",
      "validation, acc: 0.866, loss: 0.369\n",
      "validation, acc: 0.865, loss: 0.372\n",
      "validation, acc: 0.862, loss: 0.378\n",
      "validation, acc: 0.860, loss: 0.385\n",
      "validation, acc: 0.858, loss: 0.389\n",
      "validation, acc: 0.859, loss: 0.390\n",
      "validation, acc: 0.859, loss: 0.390\n",
      "validation, acc: 0.859, loss: 0.390\n",
      "validation, acc: 0.859, loss: 0.391\n",
      "validation, acc: 0.859, loss: 0.390\n",
      "validation, acc: 0.859, loss: 0.392\n",
      "validation, acc: 0.859, loss: 0.393\n",
      "validation, acc: 0.858, loss: 0.396\n",
      "validation, acc: 0.857, loss: 0.398\n",
      "validation, acc: 0.854, loss: 0.406\n",
      "validation, acc: 0.853, loss: 0.410\n",
      "validation, acc: 0.852, loss: 0.414\n",
      "validation, acc: 0.850, loss: 0.417\n",
      "validation, acc: 0.848, loss: 0.420\n",
      "validation, acc: 0.847, loss: 0.424\n",
      "validation, acc: 0.847, loss: 0.425\n",
      "validation, acc: 0.847, loss: 0.424\n",
      "validation, acc: 0.847, loss: 0.422\n",
      "validation, acc: 0.848, loss: 0.421\n",
      "validation, acc: 0.849, loss: 0.419\n",
      "validation, acc: 0.850, loss: 0.417\n",
      "validation, acc: 0.850, loss: 0.416\n",
      "validation, acc: 0.850, loss: 0.415\n",
      "validation, acc: 0.851, loss: 0.414\n",
      "epoch: 5\n",
      "step: 0, acc: 0.852, loss: 0.416 (data_loss: 0.416, reg_loss: 0.000), lr: 0.0009142439202779302\n",
      "step: 100, acc: 0.844, loss: 0.364 (data_loss: 0.364, reg_loss: 0.000), lr: 0.0009100837277029487\n",
      "step: 200, acc: 0.844, loss: 0.410 (data_loss: 0.410, reg_loss: 0.000), lr: 0.0009059612248595759\n",
      "step: 300, acc: 0.906, loss: 0.278 (data_loss: 0.278, reg_loss: 0.000), lr: 0.0009018759018759019\n",
      "step: 400, acc: 0.883, loss: 0.457 (data_loss: 0.457, reg_loss: 0.000), lr: 0.0008978272580355541\n",
      "step: 468, acc: 0.917, loss: 0.298 (data_loss: 0.298, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "training, acc: 0.869, loss: 0.364 (data_loss: 0.364, reg_loss: 0.000), lr: 0.0008950948800572861\n",
      "validation, acc: 0.953, loss: 0.118\n",
      "validation, acc: 0.965, loss: 0.100\n",
      "validation, acc: 0.956, loss: 0.119\n",
      "validation, acc: 0.955, loss: 0.118\n",
      "validation, acc: 0.963, loss: 0.104\n",
      "validation, acc: 0.965, loss: 0.099\n",
      "validation, acc: 0.964, loss: 0.119\n",
      "validation, acc: 0.960, loss: 0.133\n",
      "validation, acc: 0.941, loss: 0.180\n",
      "validation, acc: 0.925, loss: 0.216\n",
      "validation, acc: 0.917, loss: 0.244\n",
      "validation, acc: 0.909, loss: 0.262\n",
      "validation, acc: 0.898, loss: 0.300\n",
      "validation, acc: 0.893, loss: 0.311\n",
      "validation, acc: 0.888, loss: 0.321\n",
      "validation, acc: 0.884, loss: 0.331\n",
      "validation, acc: 0.889, loss: 0.318\n",
      "validation, acc: 0.891, loss: 0.308\n",
      "validation, acc: 0.894, loss: 0.303\n",
      "validation, acc: 0.896, loss: 0.294\n",
      "validation, acc: 0.898, loss: 0.288\n",
      "validation, acc: 0.899, loss: 0.282\n",
      "validation, acc: 0.899, loss: 0.279\n",
      "validation, acc: 0.896, loss: 0.287\n",
      "validation, acc: 0.887, loss: 0.314\n",
      "validation, acc: 0.877, loss: 0.336\n",
      "validation, acc: 0.870, loss: 0.349\n",
      "validation, acc: 0.862, loss: 0.367\n",
      "validation, acc: 0.855, loss: 0.384\n",
      "validation, acc: 0.848, loss: 0.401\n",
      "validation, acc: 0.841, loss: 0.416\n",
      "validation, acc: 0.844, loss: 0.413\n",
      "validation, acc: 0.847, loss: 0.404\n",
      "validation, acc: 0.850, loss: 0.399\n",
      "validation, acc: 0.854, loss: 0.389\n",
      "validation, acc: 0.857, loss: 0.382\n",
      "validation, acc: 0.859, loss: 0.375\n",
      "validation, acc: 0.861, loss: 0.372\n",
      "validation, acc: 0.864, loss: 0.370\n",
      "validation, acc: 0.865, loss: 0.365\n",
      "validation, acc: 0.867, loss: 0.360\n",
      "validation, acc: 0.870, loss: 0.354\n",
      "validation, acc: 0.872, loss: 0.348\n",
      "validation, acc: 0.874, loss: 0.344\n",
      "validation, acc: 0.876, loss: 0.339\n",
      "validation, acc: 0.878, loss: 0.335\n",
      "validation, acc: 0.880, loss: 0.333\n",
      "validation, acc: 0.876, loss: 0.341\n",
      "validation, acc: 0.874, loss: 0.346\n",
      "validation, acc: 0.872, loss: 0.352\n",
      "validation, acc: 0.870, loss: 0.356\n",
      "validation, acc: 0.870, loss: 0.359\n",
      "validation, acc: 0.867, loss: 0.365\n",
      "validation, acc: 0.865, loss: 0.371\n",
      "validation, acc: 0.863, loss: 0.375\n",
      "validation, acc: 0.864, loss: 0.376\n",
      "validation, acc: 0.864, loss: 0.377\n",
      "validation, acc: 0.864, loss: 0.377\n",
      "validation, acc: 0.864, loss: 0.378\n",
      "validation, acc: 0.864, loss: 0.377\n",
      "validation, acc: 0.864, loss: 0.379\n",
      "validation, acc: 0.865, loss: 0.379\n",
      "validation, acc: 0.863, loss: 0.382\n",
      "validation, acc: 0.862, loss: 0.385\n",
      "validation, acc: 0.859, loss: 0.392\n",
      "validation, acc: 0.858, loss: 0.397\n",
      "validation, acc: 0.857, loss: 0.401\n",
      "validation, acc: 0.855, loss: 0.404\n",
      "validation, acc: 0.853, loss: 0.407\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.851, loss: 0.413\n",
      "validation, acc: 0.851, loss: 0.412\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.852, loss: 0.410\n",
      "validation, acc: 0.853, loss: 0.409\n",
      "validation, acc: 0.853, loss: 0.407\n",
      "validation, acc: 0.854, loss: 0.406\n",
      "validation, acc: 0.854, loss: 0.405\n",
      "validation, acc: 0.854, loss: 0.405\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=5e-5),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test),\n",
    "epochs=5, batch_size=128, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb6562-da92-4cf5-a085-bbdcc39a12d7",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be6a49ff-fd28-4b4d-94b8-05b14cc4f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.086, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.727, loss: 0.696 (data_loss: 0.696, reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 200, acc: 0.812, loss: 0.649 (data_loss: 0.649, reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 300, acc: 0.781, loss: 0.628 (data_loss: 0.628, reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 400, acc: 0.812, loss: 0.536 (data_loss: 0.536, reg_loss: 0.000), lr: 0.0007142857142857143\n",
      "step: 468, acc: 0.833, loss: 0.455 (data_loss: 0.455, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.758, loss: 0.660 (data_loss: 0.660, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "validation, acc: 0.922, loss: 0.239\n",
      "validation, acc: 0.930, loss: 0.225\n",
      "validation, acc: 0.914, loss: 0.252\n",
      "validation, acc: 0.916, loss: 0.257\n",
      "validation, acc: 0.925, loss: 0.234\n",
      "validation, acc: 0.928, loss: 0.223\n",
      "validation, acc: 0.927, loss: 0.232\n",
      "validation, acc: 0.925, loss: 0.248\n",
      "validation, acc: 0.900, loss: 0.311\n",
      "validation, acc: 0.880, loss: 0.359\n",
      "validation, acc: 0.866, loss: 0.393\n",
      "validation, acc: 0.859, loss: 0.417\n",
      "validation, acc: 0.845, loss: 0.461\n",
      "validation, acc: 0.835, loss: 0.479\n",
      "validation, acc: 0.827, loss: 0.498\n",
      "validation, acc: 0.822, loss: 0.509\n",
      "validation, acc: 0.829, loss: 0.492\n",
      "validation, acc: 0.833, loss: 0.480\n",
      "validation, acc: 0.838, loss: 0.472\n",
      "validation, acc: 0.842, loss: 0.459\n",
      "validation, acc: 0.845, loss: 0.449\n",
      "validation, acc: 0.847, loss: 0.442\n",
      "validation, acc: 0.848, loss: 0.436\n",
      "validation, acc: 0.841, loss: 0.446\n",
      "validation, acc: 0.833, loss: 0.469\n",
      "validation, acc: 0.821, loss: 0.492\n",
      "validation, acc: 0.812, loss: 0.505\n",
      "validation, acc: 0.802, loss: 0.523\n",
      "validation, acc: 0.793, loss: 0.541\n",
      "validation, acc: 0.784, loss: 0.560\n",
      "validation, acc: 0.775, loss: 0.575\n",
      "validation, acc: 0.777, loss: 0.572\n",
      "validation, acc: 0.783, loss: 0.561\n",
      "validation, acc: 0.787, loss: 0.556\n",
      "validation, acc: 0.792, loss: 0.543\n",
      "validation, acc: 0.794, loss: 0.537\n",
      "validation, acc: 0.798, loss: 0.530\n",
      "validation, acc: 0.801, loss: 0.526\n",
      "validation, acc: 0.804, loss: 0.524\n",
      "validation, acc: 0.807, loss: 0.518\n",
      "validation, acc: 0.811, loss: 0.509\n",
      "validation, acc: 0.813, loss: 0.501\n",
      "validation, acc: 0.816, loss: 0.494\n",
      "validation, acc: 0.820, loss: 0.487\n",
      "validation, acc: 0.823, loss: 0.480\n",
      "validation, acc: 0.825, loss: 0.474\n",
      "validation, acc: 0.827, loss: 0.471\n",
      "validation, acc: 0.823, loss: 0.479\n",
      "validation, acc: 0.822, loss: 0.483\n",
      "validation, acc: 0.820, loss: 0.488\n",
      "validation, acc: 0.818, loss: 0.492\n",
      "validation, acc: 0.817, loss: 0.495\n",
      "validation, acc: 0.815, loss: 0.500\n",
      "validation, acc: 0.812, loss: 0.506\n",
      "validation, acc: 0.812, loss: 0.508\n",
      "validation, acc: 0.813, loss: 0.505\n",
      "validation, acc: 0.815, loss: 0.501\n",
      "validation, acc: 0.817, loss: 0.497\n",
      "validation, acc: 0.819, loss: 0.494\n",
      "validation, acc: 0.820, loss: 0.490\n",
      "validation, acc: 0.822, loss: 0.487\n",
      "validation, acc: 0.823, loss: 0.484\n",
      "validation, acc: 0.823, loss: 0.485\n",
      "validation, acc: 0.821, loss: 0.488\n",
      "validation, acc: 0.819, loss: 0.497\n",
      "validation, acc: 0.817, loss: 0.501\n",
      "validation, acc: 0.815, loss: 0.506\n",
      "validation, acc: 0.813, loss: 0.510\n",
      "validation, acc: 0.812, loss: 0.513\n",
      "validation, acc: 0.811, loss: 0.517\n",
      "validation, acc: 0.811, loss: 0.516\n",
      "validation, acc: 0.812, loss: 0.512\n",
      "validation, acc: 0.814, loss: 0.508\n",
      "validation, acc: 0.815, loss: 0.504\n",
      "validation, acc: 0.817, loss: 0.501\n",
      "validation, acc: 0.818, loss: 0.496\n",
      "validation, acc: 0.820, loss: 0.493\n",
      "validation, acc: 0.821, loss: 0.490\n",
      "validation, acc: 0.821, loss: 0.490\n",
      "epoch: 2\n",
      "step: 0, acc: 0.844, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100, acc: 0.805, loss: 0.441 (data_loss: 0.441, reg_loss: 0.000), lr: 0.0006373486297004461\n",
      "step: 200, acc: 0.820, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "step: 300, acc: 0.852, loss: 0.501 (data_loss: 0.501, reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "step: 400, acc: 0.836, loss: 0.495 (data_loss: 0.495, reg_loss: 0.000), lr: 0.0005350454788657037\n",
      "step: 468, acc: 0.865, loss: 0.388 (data_loss: 0.388, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.844, loss: 0.427 (data_loss: 0.427, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "validation, acc: 0.922, loss: 0.201\n",
      "validation, acc: 0.930, loss: 0.188\n",
      "validation, acc: 0.924, loss: 0.208\n",
      "validation, acc: 0.924, loss: 0.214\n",
      "validation, acc: 0.933, loss: 0.193\n",
      "validation, acc: 0.938, loss: 0.184\n",
      "validation, acc: 0.935, loss: 0.196\n",
      "validation, acc: 0.931, loss: 0.212\n",
      "validation, acc: 0.910, loss: 0.266\n",
      "validation, acc: 0.892, loss: 0.310\n",
      "validation, acc: 0.879, loss: 0.343\n",
      "validation, acc: 0.870, loss: 0.363\n",
      "validation, acc: 0.858, loss: 0.408\n",
      "validation, acc: 0.849, loss: 0.425\n",
      "validation, acc: 0.842, loss: 0.440\n",
      "validation, acc: 0.836, loss: 0.453\n",
      "validation, acc: 0.843, loss: 0.437\n",
      "validation, acc: 0.847, loss: 0.425\n",
      "validation, acc: 0.851, loss: 0.417\n",
      "validation, acc: 0.855, loss: 0.406\n",
      "validation, acc: 0.857, loss: 0.398\n",
      "validation, acc: 0.859, loss: 0.390\n",
      "validation, acc: 0.859, loss: 0.386\n",
      "validation, acc: 0.856, loss: 0.393\n",
      "validation, acc: 0.848, loss: 0.414\n",
      "validation, acc: 0.838, loss: 0.433\n",
      "validation, acc: 0.833, loss: 0.442\n",
      "validation, acc: 0.825, loss: 0.456\n",
      "validation, acc: 0.818, loss: 0.472\n",
      "validation, acc: 0.811, loss: 0.487\n",
      "validation, acc: 0.806, loss: 0.499\n",
      "validation, acc: 0.809, loss: 0.496\n",
      "validation, acc: 0.814, loss: 0.487\n",
      "validation, acc: 0.817, loss: 0.482\n",
      "validation, acc: 0.821, loss: 0.470\n",
      "validation, acc: 0.824, loss: 0.464\n",
      "validation, acc: 0.827, loss: 0.458\n",
      "validation, acc: 0.829, loss: 0.455\n",
      "validation, acc: 0.831, loss: 0.453\n",
      "validation, acc: 0.834, loss: 0.448\n",
      "validation, acc: 0.837, loss: 0.442\n",
      "validation, acc: 0.839, loss: 0.435\n",
      "validation, acc: 0.842, loss: 0.428\n",
      "validation, acc: 0.845, loss: 0.423\n",
      "validation, acc: 0.848, loss: 0.417\n",
      "validation, acc: 0.850, loss: 0.411\n",
      "validation, acc: 0.852, loss: 0.410\n",
      "validation, acc: 0.847, loss: 0.420\n",
      "validation, acc: 0.845, loss: 0.426\n",
      "validation, acc: 0.842, loss: 0.433\n",
      "validation, acc: 0.839, loss: 0.438\n",
      "validation, acc: 0.839, loss: 0.443\n",
      "validation, acc: 0.835, loss: 0.450\n",
      "validation, acc: 0.832, loss: 0.458\n",
      "validation, acc: 0.830, loss: 0.461\n",
      "validation, acc: 0.831, loss: 0.458\n",
      "validation, acc: 0.833, loss: 0.455\n",
      "validation, acc: 0.834, loss: 0.451\n",
      "validation, acc: 0.836, loss: 0.448\n",
      "validation, acc: 0.838, loss: 0.444\n",
      "validation, acc: 0.838, loss: 0.443\n",
      "validation, acc: 0.840, loss: 0.440\n",
      "validation, acc: 0.840, loss: 0.441\n",
      "validation, acc: 0.839, loss: 0.444\n",
      "validation, acc: 0.837, loss: 0.451\n",
      "validation, acc: 0.835, loss: 0.455\n",
      "validation, acc: 0.834, loss: 0.459\n",
      "validation, acc: 0.833, loss: 0.463\n",
      "validation, acc: 0.831, loss: 0.466\n",
      "validation, acc: 0.830, loss: 0.469\n",
      "validation, acc: 0.831, loss: 0.468\n",
      "validation, acc: 0.832, loss: 0.464\n",
      "validation, acc: 0.833, loss: 0.460\n",
      "validation, acc: 0.835, loss: 0.456\n",
      "validation, acc: 0.836, loss: 0.452\n",
      "validation, acc: 0.838, loss: 0.448\n",
      "validation, acc: 0.839, loss: 0.445\n",
      "validation, acc: 0.840, loss: 0.441\n",
      "validation, acc: 0.841, loss: 0.441\n",
      "epoch: 3\n",
      "step: 0, acc: 0.844, loss: 0.460 (data_loss: 0.460, reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 100, acc: 0.805, loss: 0.392 (data_loss: 0.392, reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "step: 200, acc: 0.844, loss: 0.423 (data_loss: 0.423, reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "step: 300, acc: 0.875, loss: 0.455 (data_loss: 0.455, reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "step: 400, acc: 0.852, loss: 0.475 (data_loss: 0.475, reg_loss: 0.000), lr: 0.00042771599657827206\n",
      "step: 468, acc: 0.896, loss: 0.349 (data_loss: 0.349, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.859, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "validation, acc: 0.922, loss: 0.239\n",
      "validation, acc: 0.922, loss: 0.228\n",
      "validation, acc: 0.914, loss: 0.255\n",
      "validation, acc: 0.912, loss: 0.257\n",
      "validation, acc: 0.920, loss: 0.234\n",
      "validation, acc: 0.923, loss: 0.223\n",
      "validation, acc: 0.920, loss: 0.240\n",
      "validation, acc: 0.917, loss: 0.254\n",
      "validation, acc: 0.904, loss: 0.292\n",
      "validation, acc: 0.891, loss: 0.323\n",
      "validation, acc: 0.882, loss: 0.345\n",
      "validation, acc: 0.877, loss: 0.357\n",
      "validation, acc: 0.867, loss: 0.394\n",
      "validation, acc: 0.864, loss: 0.403\n",
      "validation, acc: 0.859, loss: 0.413\n",
      "validation, acc: 0.856, loss: 0.421\n",
      "validation, acc: 0.862, loss: 0.404\n",
      "validation, acc: 0.867, loss: 0.390\n",
      "validation, acc: 0.872, loss: 0.380\n",
      "validation, acc: 0.876, loss: 0.367\n",
      "validation, acc: 0.879, loss: 0.358\n",
      "validation, acc: 0.882, loss: 0.349\n",
      "validation, acc: 0.885, loss: 0.343\n",
      "validation, acc: 0.881, loss: 0.351\n",
      "validation, acc: 0.869, loss: 0.376\n",
      "validation, acc: 0.858, loss: 0.398\n",
      "validation, acc: 0.853, loss: 0.409\n",
      "validation, acc: 0.843, loss: 0.425\n",
      "validation, acc: 0.835, loss: 0.443\n",
      "validation, acc: 0.827, loss: 0.460\n",
      "validation, acc: 0.821, loss: 0.474\n",
      "validation, acc: 0.823, loss: 0.471\n",
      "validation, acc: 0.827, loss: 0.461\n",
      "validation, acc: 0.830, loss: 0.456\n",
      "validation, acc: 0.834, loss: 0.445\n",
      "validation, acc: 0.837, loss: 0.439\n",
      "validation, acc: 0.839, loss: 0.432\n",
      "validation, acc: 0.841, loss: 0.429\n",
      "validation, acc: 0.844, loss: 0.427\n",
      "validation, acc: 0.846, loss: 0.422\n",
      "validation, acc: 0.849, loss: 0.416\n",
      "validation, acc: 0.852, loss: 0.409\n",
      "validation, acc: 0.854, loss: 0.403\n",
      "validation, acc: 0.857, loss: 0.398\n",
      "validation, acc: 0.860, loss: 0.392\n",
      "validation, acc: 0.862, loss: 0.387\n",
      "validation, acc: 0.863, loss: 0.385\n",
      "validation, acc: 0.858, loss: 0.395\n",
      "validation, acc: 0.857, loss: 0.400\n",
      "validation, acc: 0.854, loss: 0.407\n",
      "validation, acc: 0.852, loss: 0.412\n",
      "validation, acc: 0.851, loss: 0.416\n",
      "validation, acc: 0.848, loss: 0.422\n",
      "validation, acc: 0.845, loss: 0.430\n",
      "validation, acc: 0.843, loss: 0.432\n",
      "validation, acc: 0.844, loss: 0.430\n",
      "validation, acc: 0.845, loss: 0.427\n",
      "validation, acc: 0.846, loss: 0.424\n",
      "validation, acc: 0.847, loss: 0.422\n",
      "validation, acc: 0.849, loss: 0.418\n",
      "validation, acc: 0.849, loss: 0.417\n",
      "validation, acc: 0.851, loss: 0.415\n",
      "validation, acc: 0.850, loss: 0.416\n",
      "validation, acc: 0.851, loss: 0.418\n",
      "validation, acc: 0.848, loss: 0.425\n",
      "validation, acc: 0.847, loss: 0.428\n",
      "validation, acc: 0.846, loss: 0.432\n",
      "validation, acc: 0.845, loss: 0.435\n",
      "validation, acc: 0.844, loss: 0.437\n",
      "validation, acc: 0.843, loss: 0.440\n",
      "validation, acc: 0.844, loss: 0.439\n",
      "validation, acc: 0.844, loss: 0.436\n",
      "validation, acc: 0.846, loss: 0.432\n",
      "validation, acc: 0.847, loss: 0.429\n",
      "validation, acc: 0.848, loss: 0.425\n",
      "validation, acc: 0.850, loss: 0.421\n",
      "validation, acc: 0.851, loss: 0.418\n",
      "validation, acc: 0.852, loss: 0.415\n",
      "validation, acc: 0.852, loss: 0.415\n",
      "epoch: 4\n",
      "step: 0, acc: 0.852, loss: 0.443 (data_loss: 0.443, reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 100, acc: 0.836, loss: 0.364 (data_loss: 0.364, reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "step: 200, acc: 0.859, loss: 0.412 (data_loss: 0.412, reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "step: 300, acc: 0.875, loss: 0.426 (data_loss: 0.426, reg_loss: 0.000), lr: 0.0003694126339120798\n",
      "step: 400, acc: 0.852, loss: 0.460 (data_loss: 0.460, reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "step: 468, acc: 0.917, loss: 0.319 (data_loss: 0.319, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.868, loss: 0.362 (data_loss: 0.362, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "validation, acc: 0.922, loss: 0.255\n",
      "validation, acc: 0.922, loss: 0.244\n",
      "validation, acc: 0.911, loss: 0.274\n",
      "validation, acc: 0.912, loss: 0.273\n",
      "validation, acc: 0.914, loss: 0.250\n",
      "validation, acc: 0.919, loss: 0.237\n",
      "validation, acc: 0.914, loss: 0.258\n",
      "validation, acc: 0.912, loss: 0.271\n",
      "validation, acc: 0.902, loss: 0.301\n",
      "validation, acc: 0.891, loss: 0.326\n",
      "validation, acc: 0.883, loss: 0.344\n",
      "validation, acc: 0.881, loss: 0.352\n",
      "validation, acc: 0.870, loss: 0.384\n",
      "validation, acc: 0.870, loss: 0.389\n",
      "validation, acc: 0.867, loss: 0.397\n",
      "validation, acc: 0.864, loss: 0.403\n",
      "validation, acc: 0.870, loss: 0.386\n",
      "validation, acc: 0.875, loss: 0.371\n",
      "validation, acc: 0.880, loss: 0.360\n",
      "validation, acc: 0.884, loss: 0.348\n",
      "validation, acc: 0.886, loss: 0.339\n",
      "validation, acc: 0.889, loss: 0.329\n",
      "validation, acc: 0.891, loss: 0.322\n",
      "validation, acc: 0.886, loss: 0.332\n",
      "validation, acc: 0.874, loss: 0.359\n",
      "validation, acc: 0.862, loss: 0.382\n",
      "validation, acc: 0.856, loss: 0.394\n",
      "validation, acc: 0.846, loss: 0.411\n",
      "validation, acc: 0.838, loss: 0.430\n",
      "validation, acc: 0.829, loss: 0.448\n",
      "validation, acc: 0.823, loss: 0.463\n",
      "validation, acc: 0.825, loss: 0.459\n",
      "validation, acc: 0.830, loss: 0.450\n",
      "validation, acc: 0.832, loss: 0.445\n",
      "validation, acc: 0.836, loss: 0.434\n",
      "validation, acc: 0.839, loss: 0.427\n",
      "validation, acc: 0.841, loss: 0.421\n",
      "validation, acc: 0.843, loss: 0.418\n",
      "validation, acc: 0.846, loss: 0.415\n",
      "validation, acc: 0.848, loss: 0.410\n",
      "validation, acc: 0.851, loss: 0.404\n",
      "validation, acc: 0.853, loss: 0.397\n",
      "validation, acc: 0.856, loss: 0.391\n",
      "validation, acc: 0.859, loss: 0.386\n",
      "validation, acc: 0.861, loss: 0.380\n",
      "validation, acc: 0.864, loss: 0.374\n",
      "validation, acc: 0.865, loss: 0.372\n",
      "validation, acc: 0.861, loss: 0.382\n",
      "validation, acc: 0.859, loss: 0.387\n",
      "validation, acc: 0.856, loss: 0.393\n",
      "validation, acc: 0.854, loss: 0.398\n",
      "validation, acc: 0.853, loss: 0.402\n",
      "validation, acc: 0.850, loss: 0.408\n",
      "validation, acc: 0.848, loss: 0.415\n",
      "validation, acc: 0.846, loss: 0.418\n",
      "validation, acc: 0.847, loss: 0.416\n",
      "validation, acc: 0.848, loss: 0.413\n",
      "validation, acc: 0.850, loss: 0.410\n",
      "validation, acc: 0.851, loss: 0.408\n",
      "validation, acc: 0.852, loss: 0.405\n",
      "validation, acc: 0.853, loss: 0.403\n",
      "validation, acc: 0.854, loss: 0.402\n",
      "validation, acc: 0.854, loss: 0.402\n",
      "validation, acc: 0.854, loss: 0.404\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.851, loss: 0.414\n",
      "validation, acc: 0.851, loss: 0.417\n",
      "validation, acc: 0.849, loss: 0.420\n",
      "validation, acc: 0.848, loss: 0.422\n",
      "validation, acc: 0.847, loss: 0.425\n",
      "validation, acc: 0.847, loss: 0.424\n",
      "validation, acc: 0.848, loss: 0.421\n",
      "validation, acc: 0.850, loss: 0.417\n",
      "validation, acc: 0.851, loss: 0.414\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.853, loss: 0.407\n",
      "validation, acc: 0.855, loss: 0.404\n",
      "validation, acc: 0.856, loss: 0.401\n",
      "validation, acc: 0.856, loss: 0.400\n",
      "epoch: 5\n",
      "step: 0, acc: 0.844, loss: 0.429 (data_loss: 0.429, reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 100, acc: 0.867, loss: 0.343 (data_loss: 0.343, reg_loss: 0.000), lr: 0.00033602150537634406\n",
      "step: 200, acc: 0.859, loss: 0.399 (data_loss: 0.399, reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "step: 300, acc: 0.875, loss: 0.399 (data_loss: 0.399, reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "step: 400, acc: 0.859, loss: 0.449 (data_loss: 0.449, reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "step: 468, acc: 0.917, loss: 0.299 (data_loss: 0.299, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.875, loss: 0.345 (data_loss: 0.345, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "validation, acc: 0.922, loss: 0.248\n",
      "validation, acc: 0.922, loss: 0.238\n",
      "validation, acc: 0.909, loss: 0.269\n",
      "validation, acc: 0.912, loss: 0.268\n",
      "validation, acc: 0.917, loss: 0.245\n",
      "validation, acc: 0.923, loss: 0.232\n",
      "validation, acc: 0.917, loss: 0.255\n",
      "validation, acc: 0.916, loss: 0.267\n",
      "validation, acc: 0.905, loss: 0.295\n",
      "validation, acc: 0.895, loss: 0.316\n",
      "validation, acc: 0.891, loss: 0.332\n",
      "validation, acc: 0.889, loss: 0.338\n",
      "validation, acc: 0.880, loss: 0.368\n",
      "validation, acc: 0.880, loss: 0.372\n",
      "validation, acc: 0.876, loss: 0.379\n",
      "validation, acc: 0.874, loss: 0.384\n",
      "validation, acc: 0.879, loss: 0.367\n",
      "validation, acc: 0.884, loss: 0.353\n",
      "validation, acc: 0.888, loss: 0.342\n",
      "validation, acc: 0.891, loss: 0.330\n",
      "validation, acc: 0.894, loss: 0.321\n",
      "validation, acc: 0.896, loss: 0.312\n",
      "validation, acc: 0.899, loss: 0.305\n",
      "validation, acc: 0.893, loss: 0.316\n",
      "validation, acc: 0.880, loss: 0.345\n",
      "validation, acc: 0.867, loss: 0.370\n",
      "validation, acc: 0.861, loss: 0.383\n",
      "validation, acc: 0.851, loss: 0.401\n",
      "validation, acc: 0.842, loss: 0.422\n",
      "validation, acc: 0.833, loss: 0.440\n",
      "validation, acc: 0.826, loss: 0.457\n",
      "validation, acc: 0.827, loss: 0.453\n",
      "validation, acc: 0.831, loss: 0.443\n",
      "validation, acc: 0.834, loss: 0.438\n",
      "validation, acc: 0.838, loss: 0.427\n",
      "validation, acc: 0.841, loss: 0.420\n",
      "validation, acc: 0.844, loss: 0.414\n",
      "validation, acc: 0.846, loss: 0.411\n",
      "validation, acc: 0.848, loss: 0.408\n",
      "validation, acc: 0.851, loss: 0.403\n",
      "validation, acc: 0.853, loss: 0.397\n",
      "validation, acc: 0.856, loss: 0.390\n",
      "validation, acc: 0.858, loss: 0.384\n",
      "validation, acc: 0.861, loss: 0.379\n",
      "validation, acc: 0.864, loss: 0.373\n",
      "validation, acc: 0.866, loss: 0.367\n",
      "validation, acc: 0.868, loss: 0.365\n",
      "validation, acc: 0.863, loss: 0.374\n",
      "validation, acc: 0.861, loss: 0.379\n",
      "validation, acc: 0.859, loss: 0.385\n",
      "validation, acc: 0.857, loss: 0.390\n",
      "validation, acc: 0.856, loss: 0.394\n",
      "validation, acc: 0.853, loss: 0.400\n",
      "validation, acc: 0.850, loss: 0.407\n",
      "validation, acc: 0.849, loss: 0.410\n",
      "validation, acc: 0.850, loss: 0.407\n",
      "validation, acc: 0.852, loss: 0.405\n",
      "validation, acc: 0.853, loss: 0.402\n",
      "validation, acc: 0.854, loss: 0.399\n",
      "validation, acc: 0.855, loss: 0.396\n",
      "validation, acc: 0.856, loss: 0.395\n",
      "validation, acc: 0.857, loss: 0.393\n",
      "validation, acc: 0.857, loss: 0.394\n",
      "validation, acc: 0.857, loss: 0.395\n",
      "validation, acc: 0.856, loss: 0.401\n",
      "validation, acc: 0.855, loss: 0.404\n",
      "validation, acc: 0.854, loss: 0.406\n",
      "validation, acc: 0.853, loss: 0.409\n",
      "validation, acc: 0.852, loss: 0.411\n",
      "validation, acc: 0.851, loss: 0.413\n",
      "validation, acc: 0.852, loss: 0.412\n",
      "validation, acc: 0.853, loss: 0.409\n",
      "validation, acc: 0.854, loss: 0.406\n",
      "validation, acc: 0.855, loss: 0.403\n",
      "validation, acc: 0.856, loss: 0.400\n",
      "validation, acc: 0.857, loss: 0.396\n",
      "validation, acc: 0.858, loss: 0.393\n",
      "validation, acc: 0.860, loss: 0.390\n",
      "validation, acc: 0.860, loss: 0.390\n",
      "epoch: 6\n",
      "step: 0, acc: 0.852, loss: 0.411 (data_loss: 0.411, reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 100, acc: 0.867, loss: 0.325 (data_loss: 0.325, reg_loss: 0.000), lr: 0.00029027576197387516\n",
      "step: 200, acc: 0.859, loss: 0.385 (data_loss: 0.385, reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "step: 300, acc: 0.883, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 400, acc: 0.859, loss: 0.439 (data_loss: 0.439, reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 468, acc: 0.917, loss: 0.279 (data_loss: 0.279, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.880, loss: 0.331 (data_loss: 0.331, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "validation, acc: 0.922, loss: 0.232\n",
      "validation, acc: 0.922, loss: 0.223\n",
      "validation, acc: 0.909, loss: 0.253\n",
      "validation, acc: 0.912, loss: 0.251\n",
      "validation, acc: 0.917, loss: 0.229\n",
      "validation, acc: 0.923, loss: 0.216\n",
      "validation, acc: 0.919, loss: 0.240\n",
      "validation, acc: 0.917, loss: 0.252\n",
      "validation, acc: 0.911, loss: 0.279\n",
      "validation, acc: 0.900, loss: 0.299\n",
      "validation, acc: 0.896, loss: 0.314\n",
      "validation, acc: 0.895, loss: 0.320\n",
      "validation, acc: 0.886, loss: 0.349\n",
      "validation, acc: 0.886, loss: 0.352\n",
      "validation, acc: 0.883, loss: 0.359\n",
      "validation, acc: 0.881, loss: 0.364\n",
      "validation, acc: 0.886, loss: 0.348\n",
      "validation, acc: 0.891, loss: 0.335\n",
      "validation, acc: 0.894, loss: 0.324\n",
      "validation, acc: 0.897, loss: 0.313\n",
      "validation, acc: 0.899, loss: 0.305\n",
      "validation, acc: 0.902, loss: 0.297\n",
      "validation, acc: 0.904, loss: 0.291\n",
      "validation, acc: 0.898, loss: 0.303\n",
      "validation, acc: 0.884, loss: 0.334\n",
      "validation, acc: 0.871, loss: 0.360\n",
      "validation, acc: 0.864, loss: 0.374\n",
      "validation, acc: 0.854, loss: 0.394\n",
      "validation, acc: 0.845, loss: 0.416\n",
      "validation, acc: 0.835, loss: 0.435\n",
      "validation, acc: 0.828, loss: 0.453\n",
      "validation, acc: 0.829, loss: 0.449\n",
      "validation, acc: 0.833, loss: 0.439\n",
      "validation, acc: 0.836, loss: 0.434\n",
      "validation, acc: 0.841, loss: 0.423\n",
      "validation, acc: 0.844, loss: 0.416\n",
      "validation, acc: 0.846, loss: 0.410\n",
      "validation, acc: 0.848, loss: 0.406\n",
      "validation, acc: 0.851, loss: 0.404\n",
      "validation, acc: 0.853, loss: 0.398\n",
      "validation, acc: 0.856, loss: 0.392\n",
      "validation, acc: 0.858, loss: 0.385\n",
      "validation, acc: 0.860, loss: 0.379\n",
      "validation, acc: 0.863, loss: 0.374\n",
      "validation, acc: 0.866, loss: 0.368\n",
      "validation, acc: 0.868, loss: 0.362\n",
      "validation, acc: 0.869, loss: 0.360\n",
      "validation, acc: 0.865, loss: 0.369\n",
      "validation, acc: 0.863, loss: 0.374\n",
      "validation, acc: 0.861, loss: 0.380\n",
      "validation, acc: 0.859, loss: 0.385\n",
      "validation, acc: 0.858, loss: 0.388\n",
      "validation, acc: 0.855, loss: 0.394\n",
      "validation, acc: 0.853, loss: 0.400\n",
      "validation, acc: 0.852, loss: 0.403\n",
      "validation, acc: 0.853, loss: 0.401\n",
      "validation, acc: 0.854, loss: 0.398\n",
      "validation, acc: 0.855, loss: 0.395\n",
      "validation, acc: 0.857, loss: 0.393\n",
      "validation, acc: 0.858, loss: 0.390\n",
      "validation, acc: 0.859, loss: 0.389\n",
      "validation, acc: 0.860, loss: 0.387\n",
      "validation, acc: 0.860, loss: 0.387\n",
      "validation, acc: 0.860, loss: 0.388\n",
      "validation, acc: 0.859, loss: 0.394\n",
      "validation, acc: 0.858, loss: 0.397\n",
      "validation, acc: 0.857, loss: 0.398\n",
      "validation, acc: 0.856, loss: 0.401\n",
      "validation, acc: 0.855, loss: 0.402\n",
      "validation, acc: 0.854, loss: 0.404\n",
      "validation, acc: 0.855, loss: 0.403\n",
      "validation, acc: 0.856, loss: 0.400\n",
      "validation, acc: 0.857, loss: 0.397\n",
      "validation, acc: 0.858, loss: 0.394\n",
      "validation, acc: 0.859, loss: 0.391\n",
      "validation, acc: 0.860, loss: 0.387\n",
      "validation, acc: 0.861, loss: 0.385\n",
      "validation, acc: 0.862, loss: 0.382\n",
      "validation, acc: 0.863, loss: 0.381\n",
      "epoch: 7\n",
      "step: 0, acc: 0.852, loss: 0.393 (data_loss: 0.393, reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 100, acc: 0.859, loss: 0.312 (data_loss: 0.312, reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "step: 200, acc: 0.859, loss: 0.372 (data_loss: 0.372, reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "step: 300, acc: 0.883, loss: 0.355 (data_loss: 0.355, reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "step: 400, acc: 0.859, loss: 0.433 (data_loss: 0.433, reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "step: 468, acc: 0.927, loss: 0.260 (data_loss: 0.260, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.884, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "validation, acc: 0.945, loss: 0.218\n",
      "validation, acc: 0.938, loss: 0.210\n",
      "validation, acc: 0.919, loss: 0.239\n",
      "validation, acc: 0.922, loss: 0.238\n",
      "validation, acc: 0.927, loss: 0.216\n",
      "validation, acc: 0.931, loss: 0.204\n",
      "validation, acc: 0.924, loss: 0.228\n",
      "validation, acc: 0.923, loss: 0.240\n",
      "validation, acc: 0.915, loss: 0.268\n",
      "validation, acc: 0.904, loss: 0.289\n",
      "validation, acc: 0.899, loss: 0.304\n",
      "validation, acc: 0.898, loss: 0.309\n",
      "validation, acc: 0.890, loss: 0.338\n",
      "validation, acc: 0.890, loss: 0.341\n",
      "validation, acc: 0.887, loss: 0.348\n",
      "validation, acc: 0.885, loss: 0.353\n",
      "validation, acc: 0.890, loss: 0.338\n",
      "validation, acc: 0.895, loss: 0.325\n",
      "validation, acc: 0.897, loss: 0.315\n",
      "validation, acc: 0.901, loss: 0.304\n",
      "validation, acc: 0.903, loss: 0.296\n",
      "validation, acc: 0.906, loss: 0.287\n",
      "validation, acc: 0.908, loss: 0.282\n",
      "validation, acc: 0.902, loss: 0.295\n",
      "validation, acc: 0.887, loss: 0.326\n",
      "validation, acc: 0.875, loss: 0.353\n",
      "validation, acc: 0.867, loss: 0.368\n",
      "validation, acc: 0.857, loss: 0.388\n",
      "validation, acc: 0.848, loss: 0.410\n",
      "validation, acc: 0.838, loss: 0.430\n",
      "validation, acc: 0.830, loss: 0.448\n",
      "validation, acc: 0.831, loss: 0.444\n",
      "validation, acc: 0.835, loss: 0.434\n",
      "validation, acc: 0.838, loss: 0.429\n",
      "validation, acc: 0.843, loss: 0.418\n",
      "validation, acc: 0.845, loss: 0.411\n",
      "validation, acc: 0.848, loss: 0.405\n",
      "validation, acc: 0.850, loss: 0.401\n",
      "validation, acc: 0.852, loss: 0.398\n",
      "validation, acc: 0.855, loss: 0.393\n",
      "validation, acc: 0.857, loss: 0.386\n",
      "validation, acc: 0.860, loss: 0.379\n",
      "validation, acc: 0.863, loss: 0.373\n",
      "validation, acc: 0.865, loss: 0.369\n",
      "validation, acc: 0.868, loss: 0.363\n",
      "validation, acc: 0.870, loss: 0.357\n",
      "validation, acc: 0.872, loss: 0.355\n",
      "validation, acc: 0.868, loss: 0.364\n",
      "validation, acc: 0.866, loss: 0.368\n",
      "validation, acc: 0.864, loss: 0.374\n",
      "validation, acc: 0.861, loss: 0.379\n",
      "validation, acc: 0.860, loss: 0.381\n",
      "validation, acc: 0.858, loss: 0.387\n",
      "validation, acc: 0.855, loss: 0.394\n",
      "validation, acc: 0.855, loss: 0.396\n",
      "validation, acc: 0.856, loss: 0.394\n",
      "validation, acc: 0.857, loss: 0.391\n",
      "validation, acc: 0.858, loss: 0.388\n",
      "validation, acc: 0.860, loss: 0.386\n",
      "validation, acc: 0.861, loss: 0.383\n",
      "validation, acc: 0.861, loss: 0.382\n",
      "validation, acc: 0.862, loss: 0.380\n",
      "validation, acc: 0.862, loss: 0.380\n",
      "validation, acc: 0.863, loss: 0.382\n",
      "validation, acc: 0.861, loss: 0.387\n",
      "validation, acc: 0.860, loss: 0.390\n",
      "validation, acc: 0.860, loss: 0.391\n",
      "validation, acc: 0.859, loss: 0.393\n",
      "validation, acc: 0.859, loss: 0.394\n",
      "validation, acc: 0.858, loss: 0.396\n",
      "validation, acc: 0.858, loss: 0.396\n",
      "validation, acc: 0.859, loss: 0.392\n",
      "validation, acc: 0.860, loss: 0.389\n",
      "validation, acc: 0.861, loss: 0.387\n",
      "validation, acc: 0.862, loss: 0.384\n",
      "validation, acc: 0.863, loss: 0.380\n",
      "validation, acc: 0.864, loss: 0.377\n",
      "validation, acc: 0.865, loss: 0.374\n",
      "validation, acc: 0.866, loss: 0.374\n",
      "epoch: 8\n",
      "step: 0, acc: 0.859, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 100, acc: 0.883, loss: 0.304 (data_loss: 0.304, reg_loss: 0.000), lr: 0.00022815423226100847\n",
      "step: 200, acc: 0.867, loss: 0.361 (data_loss: 0.361, reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "step: 300, acc: 0.883, loss: 0.341 (data_loss: 0.341, reg_loss: 0.000), lr: 0.00021819768710451667\n",
      "step: 400, acc: 0.859, loss: 0.428 (data_loss: 0.428, reg_loss: 0.000), lr: 0.00021353833013025838\n",
      "step: 468, acc: 0.948, loss: 0.244 (data_loss: 0.244, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.887, loss: 0.312 (data_loss: 0.312, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "validation, acc: 0.945, loss: 0.203\n",
      "validation, acc: 0.938, loss: 0.195\n",
      "validation, acc: 0.927, loss: 0.223\n",
      "validation, acc: 0.928, loss: 0.222\n",
      "validation, acc: 0.933, loss: 0.200\n",
      "validation, acc: 0.936, loss: 0.189\n",
      "validation, acc: 0.929, loss: 0.214\n",
      "validation, acc: 0.927, loss: 0.226\n",
      "validation, acc: 0.918, loss: 0.256\n",
      "validation, acc: 0.908, loss: 0.278\n",
      "validation, acc: 0.902, loss: 0.294\n",
      "validation, acc: 0.900, loss: 0.301\n",
      "validation, acc: 0.891, loss: 0.330\n",
      "validation, acc: 0.891, loss: 0.334\n",
      "validation, acc: 0.887, loss: 0.342\n",
      "validation, acc: 0.886, loss: 0.347\n",
      "validation, acc: 0.891, loss: 0.331\n",
      "validation, acc: 0.896, loss: 0.318\n",
      "validation, acc: 0.898, loss: 0.308\n",
      "validation, acc: 0.902, loss: 0.297\n",
      "validation, acc: 0.904, loss: 0.290\n",
      "validation, acc: 0.907, loss: 0.281\n",
      "validation, acc: 0.909, loss: 0.276\n",
      "validation, acc: 0.904, loss: 0.288\n",
      "validation, acc: 0.889, loss: 0.320\n",
      "validation, acc: 0.877, loss: 0.346\n",
      "validation, acc: 0.870, loss: 0.361\n",
      "validation, acc: 0.860, loss: 0.381\n",
      "validation, acc: 0.852, loss: 0.402\n",
      "validation, acc: 0.843, loss: 0.422\n",
      "validation, acc: 0.835, loss: 0.440\n",
      "validation, acc: 0.837, loss: 0.435\n",
      "validation, acc: 0.841, loss: 0.426\n",
      "validation, acc: 0.844, loss: 0.420\n",
      "validation, acc: 0.848, loss: 0.410\n",
      "validation, acc: 0.851, loss: 0.403\n",
      "validation, acc: 0.853, loss: 0.397\n",
      "validation, acc: 0.856, loss: 0.393\n",
      "validation, acc: 0.858, loss: 0.390\n",
      "validation, acc: 0.860, loss: 0.385\n",
      "validation, acc: 0.863, loss: 0.379\n",
      "validation, acc: 0.865, loss: 0.372\n",
      "validation, acc: 0.868, loss: 0.366\n",
      "validation, acc: 0.870, loss: 0.361\n",
      "validation, acc: 0.873, loss: 0.355\n",
      "validation, acc: 0.875, loss: 0.350\n",
      "validation, acc: 0.876, loss: 0.348\n",
      "validation, acc: 0.873, loss: 0.356\n",
      "validation, acc: 0.871, loss: 0.360\n",
      "validation, acc: 0.869, loss: 0.366\n",
      "validation, acc: 0.866, loss: 0.371\n",
      "validation, acc: 0.865, loss: 0.373\n",
      "validation, acc: 0.863, loss: 0.379\n",
      "validation, acc: 0.860, loss: 0.385\n",
      "validation, acc: 0.859, loss: 0.387\n",
      "validation, acc: 0.860, loss: 0.385\n",
      "validation, acc: 0.862, loss: 0.383\n",
      "validation, acc: 0.863, loss: 0.380\n",
      "validation, acc: 0.864, loss: 0.378\n",
      "validation, acc: 0.865, loss: 0.375\n",
      "validation, acc: 0.866, loss: 0.374\n",
      "validation, acc: 0.867, loss: 0.372\n",
      "validation, acc: 0.867, loss: 0.372\n",
      "validation, acc: 0.867, loss: 0.374\n",
      "validation, acc: 0.865, loss: 0.379\n",
      "validation, acc: 0.864, loss: 0.382\n",
      "validation, acc: 0.864, loss: 0.383\n",
      "validation, acc: 0.863, loss: 0.385\n",
      "validation, acc: 0.862, loss: 0.387\n",
      "validation, acc: 0.861, loss: 0.389\n",
      "validation, acc: 0.862, loss: 0.389\n",
      "validation, acc: 0.863, loss: 0.386\n",
      "validation, acc: 0.864, loss: 0.383\n",
      "validation, acc: 0.864, loss: 0.380\n",
      "validation, acc: 0.866, loss: 0.377\n",
      "validation, acc: 0.867, loss: 0.374\n",
      "validation, acc: 0.868, loss: 0.371\n",
      "validation, acc: 0.869, loss: 0.368\n",
      "validation, acc: 0.869, loss: 0.368\n",
      "epoch: 9\n",
      "step: 0, acc: 0.867, loss: 0.365 (data_loss: 0.365, reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 100, acc: 0.883, loss: 0.299 (data_loss: 0.299, reg_loss: 0.000), lr: 0.0002061005770816158\n",
      "step: 200, acc: 0.867, loss: 0.351 (data_loss: 0.351, reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "step: 300, acc: 0.883, loss: 0.329 (data_loss: 0.329, reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "step: 400, acc: 0.867, loss: 0.422 (data_loss: 0.422, reg_loss: 0.000), lr: 0.0001940993788819876\n",
      "step: 468, acc: 0.958, loss: 0.232 (data_loss: 0.232, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.890, loss: 0.304 (data_loss: 0.304, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "validation, acc: 0.945, loss: 0.187\n",
      "validation, acc: 0.938, loss: 0.181\n",
      "validation, acc: 0.932, loss: 0.207\n",
      "validation, acc: 0.932, loss: 0.207\n",
      "validation, acc: 0.938, loss: 0.186\n",
      "validation, acc: 0.940, loss: 0.176\n",
      "validation, acc: 0.934, loss: 0.201\n",
      "validation, acc: 0.932, loss: 0.213\n",
      "validation, acc: 0.922, loss: 0.244\n",
      "validation, acc: 0.912, loss: 0.267\n",
      "validation, acc: 0.906, loss: 0.284\n",
      "validation, acc: 0.904, loss: 0.291\n",
      "validation, acc: 0.894, loss: 0.321\n",
      "validation, acc: 0.893, loss: 0.325\n",
      "validation, acc: 0.890, loss: 0.334\n",
      "validation, acc: 0.888, loss: 0.339\n",
      "validation, acc: 0.893, loss: 0.324\n",
      "validation, acc: 0.898, loss: 0.311\n",
      "validation, acc: 0.900, loss: 0.301\n",
      "validation, acc: 0.904, loss: 0.291\n",
      "validation, acc: 0.906, loss: 0.283\n",
      "validation, acc: 0.908, loss: 0.275\n",
      "validation, acc: 0.910, loss: 0.270\n",
      "validation, acc: 0.905, loss: 0.282\n",
      "validation, acc: 0.891, loss: 0.313\n",
      "validation, acc: 0.880, loss: 0.339\n",
      "validation, acc: 0.873, loss: 0.353\n",
      "validation, acc: 0.862, loss: 0.373\n",
      "validation, acc: 0.855, loss: 0.394\n",
      "validation, acc: 0.846, loss: 0.413\n",
      "validation, acc: 0.839, loss: 0.430\n",
      "validation, acc: 0.840, loss: 0.426\n",
      "validation, acc: 0.844, loss: 0.417\n",
      "validation, acc: 0.847, loss: 0.411\n",
      "validation, acc: 0.851, loss: 0.401\n",
      "validation, acc: 0.854, loss: 0.395\n",
      "validation, acc: 0.856, loss: 0.388\n",
      "validation, acc: 0.858, loss: 0.385\n",
      "validation, acc: 0.860, loss: 0.382\n",
      "validation, acc: 0.863, loss: 0.377\n",
      "validation, acc: 0.865, loss: 0.370\n",
      "validation, acc: 0.868, loss: 0.364\n",
      "validation, acc: 0.870, loss: 0.358\n",
      "validation, acc: 0.873, loss: 0.353\n",
      "validation, acc: 0.875, loss: 0.347\n",
      "validation, acc: 0.877, loss: 0.342\n",
      "validation, acc: 0.879, loss: 0.340\n",
      "validation, acc: 0.875, loss: 0.348\n",
      "validation, acc: 0.874, loss: 0.352\n",
      "validation, acc: 0.871, loss: 0.358\n",
      "validation, acc: 0.869, loss: 0.363\n",
      "validation, acc: 0.867, loss: 0.365\n",
      "validation, acc: 0.865, loss: 0.371\n",
      "validation, acc: 0.862, loss: 0.376\n",
      "validation, acc: 0.862, loss: 0.379\n",
      "validation, acc: 0.862, loss: 0.377\n",
      "validation, acc: 0.864, loss: 0.375\n",
      "validation, acc: 0.865, loss: 0.372\n",
      "validation, acc: 0.866, loss: 0.370\n",
      "validation, acc: 0.867, loss: 0.368\n",
      "validation, acc: 0.867, loss: 0.367\n",
      "validation, acc: 0.868, loss: 0.365\n",
      "validation, acc: 0.868, loss: 0.366\n",
      "validation, acc: 0.869, loss: 0.367\n",
      "validation, acc: 0.867, loss: 0.373\n",
      "validation, acc: 0.865, loss: 0.376\n",
      "validation, acc: 0.865, loss: 0.377\n",
      "validation, acc: 0.864, loss: 0.379\n",
      "validation, acc: 0.863, loss: 0.381\n",
      "validation, acc: 0.862, loss: 0.383\n",
      "validation, acc: 0.863, loss: 0.382\n",
      "validation, acc: 0.863, loss: 0.379\n",
      "validation, acc: 0.865, loss: 0.377\n",
      "validation, acc: 0.866, loss: 0.374\n",
      "validation, acc: 0.867, loss: 0.371\n",
      "validation, acc: 0.868, loss: 0.368\n",
      "validation, acc: 0.869, loss: 0.366\n",
      "validation, acc: 0.870, loss: 0.363\n",
      "validation, acc: 0.870, loss: 0.362\n",
      "epoch: 10\n",
      "step: 0, acc: 0.867, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 100, acc: 0.883, loss: 0.291 (data_loss: 0.291, reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "step: 200, acc: 0.867, loss: 0.346 (data_loss: 0.346, reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "step: 300, acc: 0.883, loss: 0.319 (data_loss: 0.319, reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "step: 400, acc: 0.875, loss: 0.414 (data_loss: 0.414, reg_loss: 0.000), lr: 0.00017790428749332856\n",
      "step: 468, acc: 0.958, loss: 0.224 (data_loss: 0.224, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.892, loss: 0.297 (data_loss: 0.297, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "validation, acc: 0.953, loss: 0.179\n",
      "validation, acc: 0.953, loss: 0.172\n",
      "validation, acc: 0.943, loss: 0.198\n",
      "validation, acc: 0.939, loss: 0.197\n",
      "validation, acc: 0.944, loss: 0.177\n",
      "validation, acc: 0.947, loss: 0.167\n",
      "validation, acc: 0.941, loss: 0.192\n",
      "validation, acc: 0.938, loss: 0.204\n",
      "validation, acc: 0.925, loss: 0.236\n",
      "validation, acc: 0.914, loss: 0.260\n",
      "validation, acc: 0.908, loss: 0.277\n",
      "validation, acc: 0.906, loss: 0.285\n",
      "validation, acc: 0.896, loss: 0.314\n",
      "validation, acc: 0.894, loss: 0.320\n",
      "validation, acc: 0.891, loss: 0.328\n",
      "validation, acc: 0.889, loss: 0.334\n",
      "validation, acc: 0.894, loss: 0.319\n",
      "validation, acc: 0.898, loss: 0.307\n",
      "validation, acc: 0.901, loss: 0.297\n",
      "validation, acc: 0.904, loss: 0.287\n",
      "validation, acc: 0.906, loss: 0.280\n",
      "validation, acc: 0.909, loss: 0.272\n",
      "validation, acc: 0.911, loss: 0.267\n",
      "validation, acc: 0.906, loss: 0.279\n",
      "validation, acc: 0.892, loss: 0.309\n",
      "validation, acc: 0.881, loss: 0.334\n",
      "validation, acc: 0.874, loss: 0.347\n",
      "validation, acc: 0.864, loss: 0.366\n",
      "validation, acc: 0.857, loss: 0.386\n",
      "validation, acc: 0.849, loss: 0.405\n",
      "validation, acc: 0.842, loss: 0.422\n",
      "validation, acc: 0.844, loss: 0.417\n",
      "validation, acc: 0.848, loss: 0.408\n",
      "validation, acc: 0.850, loss: 0.403\n",
      "validation, acc: 0.854, loss: 0.393\n",
      "validation, acc: 0.857, loss: 0.387\n",
      "validation, acc: 0.859, loss: 0.380\n",
      "validation, acc: 0.861, loss: 0.377\n",
      "validation, acc: 0.863, loss: 0.374\n",
      "validation, acc: 0.866, loss: 0.370\n",
      "validation, acc: 0.868, loss: 0.363\n",
      "validation, acc: 0.871, loss: 0.357\n",
      "validation, acc: 0.873, loss: 0.351\n",
      "validation, acc: 0.875, loss: 0.347\n",
      "validation, acc: 0.878, loss: 0.341\n",
      "validation, acc: 0.880, loss: 0.336\n",
      "validation, acc: 0.881, loss: 0.334\n",
      "validation, acc: 0.878, loss: 0.342\n",
      "validation, acc: 0.877, loss: 0.346\n",
      "validation, acc: 0.874, loss: 0.351\n",
      "validation, acc: 0.871, loss: 0.356\n",
      "validation, acc: 0.870, loss: 0.358\n",
      "validation, acc: 0.867, loss: 0.364\n",
      "validation, acc: 0.865, loss: 0.370\n",
      "validation, acc: 0.864, loss: 0.373\n",
      "validation, acc: 0.865, loss: 0.371\n",
      "validation, acc: 0.866, loss: 0.369\n",
      "validation, acc: 0.867, loss: 0.366\n",
      "validation, acc: 0.868, loss: 0.365\n",
      "validation, acc: 0.869, loss: 0.362\n",
      "validation, acc: 0.869, loss: 0.362\n",
      "validation, acc: 0.870, loss: 0.360\n",
      "validation, acc: 0.870, loss: 0.361\n",
      "validation, acc: 0.871, loss: 0.362\n",
      "validation, acc: 0.868, loss: 0.368\n",
      "validation, acc: 0.867, loss: 0.371\n",
      "validation, acc: 0.867, loss: 0.372\n",
      "validation, acc: 0.866, loss: 0.374\n",
      "validation, acc: 0.865, loss: 0.376\n",
      "validation, acc: 0.864, loss: 0.378\n",
      "validation, acc: 0.864, loss: 0.378\n",
      "validation, acc: 0.865, loss: 0.375\n",
      "validation, acc: 0.866, loss: 0.372\n",
      "validation, acc: 0.867, loss: 0.370\n",
      "validation, acc: 0.868, loss: 0.367\n",
      "validation, acc: 0.870, loss: 0.364\n",
      "validation, acc: 0.871, loss: 0.361\n",
      "validation, acc: 0.872, loss: 0.358\n",
      "validation, acc: 0.872, loss: 0.358\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "# Instantiate the model\n",
    "model = Model()\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=1e-3),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test), epochs=10, batch_size=128, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f95189ad-699b-4fa3-90db-65aceb4d7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1772e5d-2cb7-42e1-8b36-52ad827d1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_parameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2bd82e9-a3b1-43c2-8f63-addb185374ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.872, loss: 0.358\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb5b98-b065-48ed-bab2-be5f3e5eac27",
   "metadata": {},
   "source": [
    "# Saving Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b089aaad-49ab-46a7-9a10-170a04647998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e641e992-d772-4c1d-8b51-dc93a82ebec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('fashion_mnist.parms')\n",
    "\n",
    "model.save('fashion_mnist.model')\n",
    "\n",
    "model = Model.load('fashion_mnist.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831703a-6bc6-43c9-a9e8-ac011e5f7388",
   "metadata": {},
   "source": [
    "# Prediction/Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c127a5e-e6db-49b1-a926-f2ee971e2fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.3235634e-06 5.4504189e-06 1.3792906e-07 1.8784121e-06 1.9941680e-08 2.9539813e-03 1.6619454e-07 1.0636183e-03 5.0342771e-07 9.9597186e-01]\n",
      " [2.0049891e-09 4.0154222e-09 9.8212341e-11 1.4655454e-08 1.1969134e-10 8.6525673e-05 1.6515129e-09 1.5844597e-02 5.3046992e-06 9.8406351e-01]\n",
      " [2.8690608e-06 1.2095607e-06 3.2037498e-08 9.9404690e-07 1.6935587e-09 1.1243765e-02 6.6337069e-09 8.3305147e-03 1.4526491e-05 9.8040605e-01]\n",
      " [1.6122308e-06 2.4565222e-07 2.0918116e-09 9.4786344e-08 7.5437510e-11 3.2022949e-02 1.8768553e-09 1.4054976e-02 2.1136093e-05 9.5389903e-01]\n",
      " [1.7426168e-09 6.6433326e-10 4.4490895e-12 1.5621661e-10 2.2314140e-13 2.3109384e-05 8.4007063e-12 4.1622648e-04 2.5635080e-08 9.9956065e-01]\n",
      " [2.6979894e-09 2.4188958e-09 1.4101793e-12 1.3416060e-10 4.0655866e-13 1.7546439e-03 6.2037923e-11 2.7516185e-06 3.8930695e-10 9.9824262e-01]\n",
      " [6.2281214e-07 1.6973397e-07 7.1548605e-08 3.8755832e-07 2.3197160e-09 7.5783953e-04 3.7351374e-08 2.7741536e-02 4.7824788e-06 9.7149456e-01]\n",
      " [4.4357243e-08 3.0270773e-08 2.8484135e-09 1.5338645e-07 5.5315041e-10 2.1440664e-04 4.0838466e-09 2.2931457e-01 9.8225530e-05 7.7037257e-01]\n",
      " [1.2420795e-08 1.1683668e-08 5.6829436e-10 6.3484762e-08 2.8557323e-10 5.5646192e-04 1.1744882e-09 4.8012882e-01 1.4833020e-04 5.1916629e-01]\n",
      " [1.0526977e-06 9.2412836e-07 6.5974723e-07 2.8577711e-06 6.2819630e-08 1.5944059e-03 2.5016070e-07 9.6498877e-01 1.8558296e-04 3.3225376e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 samples from validation dataset\n",
    "# and print the result\n",
    "confidences = model.predict(X_test[:10])\n",
    "print(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a51d752d-2cbe-4031-9f56-a7ed89bca4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 9 9 9 9 9 9 9 9 7]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.output_layer_activation.predictions(confidences)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "409aa36d-b423-481f-ba44-ec161bc4cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 9 9 9 9 9 9 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 labels\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8c3ae38-7a83-46ea-a53d-aa1060985b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label index to label name relation\n",
    "fashion_mnist_labels = {\n",
    "0: 'T-shirt/top',\n",
    "1: 'Trouser',\n",
    "2: 'Pullover',\n",
    "3: 'Dress',\n",
    "4: 'Coat',\n",
    "5: 'Sandal',\n",
    "6: 'Shirt',\n",
    "7: 'Sneaker',\n",
    "8: 'Bag',\n",
    "9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7177fa64-d7ed-473f-8b51-16599962bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Ankle boot\n",
      "Sneaker\n"
     ]
    }
   ],
   "source": [
    "for pic in predictions:\n",
    "    print(fashion_mnist_labels[pic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec7754-9fa4-42d0-bf4c-913fc62ee456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
